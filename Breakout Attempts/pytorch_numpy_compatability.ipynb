{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obs(obs):\n",
    "    temp = np.copy(obs)\n",
    "    temp = temp[31:193, 7:152]\n",
    "    temp = np.dot(temp[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "    temp = temp / 148.3038\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 145)\n",
      "<class 'tuple'>\n",
      "(15, 162, 145)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs = preprocess_obs(obs)\n",
    "print(obs.shape)\n",
    "print(type(obs.shape))\n",
    "print((42,*obs.shape))\n",
    "print(type(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs = preprocess_obs(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "    def __init__(self,maxlen, n_actions, state_dim):\n",
    "        self.maxlen = maxlen\n",
    "        self.n_actions = n_actions\n",
    "        self.state_memory = np.zeros((maxlen, *state_dim),dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((maxlen, *state_dim),dtype=np.float32)\n",
    "        self.action_memory = np.zeros((maxlen, n_actions),dtype=np.int32)\n",
    "        self.reward_memory = np.zeros((maxlen),dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros((maxlen),dtype=np.uint8)\n",
    "        self.mem_counter = 0\n",
    "    def store_transition(self,state,action,reward,state_,terminal):\n",
    "        index = self.mem_counter % self.maxlen\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        actions = np.zeros(self.n_actions)\n",
    "        actions[action] = 1.0\n",
    "        self.action_memory[index] = actions\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - terminal\n",
    "        self.mem_counter+=1\n",
    "    def sample_memory(self,batch_size):\n",
    "        mem_size = self.mem_counter if self.mem_counter < self.maxlen else self.maxlen\n",
    "        batch = np.random.choice(mem_size,batch_size,replace=False)\n",
    "        state_batch = self.state_memory[batch]\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = self.reward_memory[batch]\n",
    "        terminal_batch = self.terminal_memory[batch]\n",
    "        new_state_batch = self.new_state_memory[batch]\n",
    "        return (state_batch,action_batch,reward_batch,new_state_batch,terminal_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "(10, 162, 145)\n",
      "(10, 4)\n",
      "(10,)\n",
      "(10, 162, 145)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "buffer = MemoryBuffer(20,4,(162,145))\n",
    "current = env.reset()\n",
    "current = preprocess_obs(current)\n",
    "for i in range(25):\n",
    "    new_state, reward, done, info = env.step(0)\n",
    "    new_state = preprocess_obs(new_state)\n",
    "    buffer.store_transition(current,0,reward,new_state,done)\n",
    "print(buffer.mem_counter)\n",
    "state_batch, action_batch, reward_batch, new_state_batch, terminal_batch = buffer.sample_memory(10) \n",
    "print(state_batch.shape)\n",
    "print(action_batch.shape)\n",
    "print(reward_batch.shape)\n",
    "print(new_state_batch.shape)\n",
    "print(terminal_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,5)\n",
    "        self.conv2 = nn.Conv2d(32,64,5)\n",
    "        self.conv3 = nn.Conv2d(64,128,5)\n",
    "        self.fc1 = nn.Linear(28672,64)\n",
    "        self.fc2 = nn.Linear(64,4)\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
    "        x = x.view(-1,28672)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 162, 145])\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "net.zero_grad()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "state_batch_tensor = torch.Tensor(state_batch)\n",
    "print(state_batch_tensor.shape)\n",
    "action_batch_tensor = torch.Tensor(action_batch)\n",
    "state_batch_tensor=state_batch_tensor.view(-1,1,162,145)\n",
    "output = net.forward(state_batch_tensor)\n",
    "print(output.shape)\n",
    "loss = nn.MSELoss()\n",
    "print(action_batch_tensor.shape)\n",
    "loss = loss(output,action_batch_tensor)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 score = 0\n",
      "()\n",
      "(64, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [64], [64, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-213-af7af4d4da03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mscore\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobs_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-209-609285a7296d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mstuff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_next\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstuff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mq_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_indices\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_batch_tensor\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_next\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mterminal_batch_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps_decay\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps_min\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps_min\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [64], [64, 4]"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-v0\")\n",
    "agent = Agent()\n",
    "scores = []\n",
    "n_games = 500\n",
    "score = 0\n",
    "eps_history = []\n",
    "\n",
    "for i in range(n_games):\n",
    "    print(f'episode: {i} score = {score}')\n",
    "    score = 0\n",
    "    eps_history.append(agent.epsilon)\n",
    "    obs = preprocess_obs(env.reset())\n",
    "    done = False \n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        obs_, reward, done, info = env.step(action)\n",
    "        obs_ = preprocess_obs(obs_)\n",
    "        score+=reward\n",
    "        agent.memory.store_transition(obs,action,reward,obs_,done)\n",
    "        agent.train()\n",
    "        obs = obs_\n",
    "    score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    epsilon = 1\n",
    "    eps_decay = 0.9998\n",
    "    eps_min = 0.007\n",
    "    batch_size = 64\n",
    "    obs_dim = (162,145)\n",
    "    gamma = 0.99\n",
    "    def __init__(self):\n",
    "        self.model = Net()\n",
    "        self.optimzer = optim.Adam(self.model.parameters(),lr=0.001)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.memory = MemoryBuffer(20000,4,self.obs_dim)\n",
    "    def train(self):\n",
    "        if self.memory.mem_counter > self.batch_size:\n",
    "            state_batch, action_batch, reward_batch, new_state_batch, terminal_batch = self.memory.sample_memory(self.batch_size)\n",
    "            \n",
    "            action_values = np.array(4,dtype=np.uint8)\n",
    "            action_indices = np.dot(action_batch, action_values)\n",
    "            \n",
    "            print(action_values.shape)\n",
    "            print(action_indices.shape)\n",
    "            \n",
    "            state_batch_tensor = torch.from_numpy(state_batch).view(-1,1,*self.obs_dim)\n",
    "            new_state_batch_tensor = torch.from_numpy(new_state_batch).view(-1,1,*self.obs_dim)\n",
    "            reward_batch_tensor = torch.from_numpy(reward_batch)\n",
    "            terminal_batch_tensor = torch.from_numpy(terminal_batch)\n",
    "            \n",
    "            q_eval = self.model.forward(state_batch_tensor)\n",
    "            q_target = self.model.forward(state_batch_tensor)\n",
    "            q_next = self.model.forward(new_state_batch_tensor)\n",
    "            \n",
    "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "            \n",
    "            print(reward_batch_tensor.shape)\n",
    "            print(terminal_batch_tensor.shape)\n",
    "            stuff = torch.max(q_next,dim=1)[0]\n",
    "            print(stuff.shape)\n",
    "            q_target[batch_index,action_indices] = reward_batch_tensor + self.gamma*torch.max(q_next,dim=1)[0]*terminal_batch_tensor\n",
    "            self.epsilon = self.epsilon * self.eps_decay if self.epsilon > self.eps_min else self.eps_min\n",
    "            loss = self.loss(q_target,q_eval)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    def choose_action(self,obs):\n",
    "        rand = np.random.random()\n",
    "        if rand < self.epsilon:\n",
    "            action = np.random.choice(4)\n",
    "        else:\n",
    "            actions = self.model.forward(obs)\n",
    "            action = torch.argmax(actions).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
