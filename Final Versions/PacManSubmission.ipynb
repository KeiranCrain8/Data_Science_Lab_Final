{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Preprocesses a 210x160x3 frame to 84x84x1 grayscale\n",
    "#Arguments:\n",
    "#   frame: The frame to process.  Must have values ranging from 0-255\n",
    "#Returns:\n",
    "#   The processed frame\n",
    "def process_frame(frame, shape=(84, 84)):\n",
    "    frame = frame.astype(np.uint8)  # cv2 requires np.uint8, other dtypes will not work\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = frame[34:34+160, :160]  # crop image\n",
    "    frame = cv2.resize(frame, shape, interpolation=cv2.INTER_NEAREST)\n",
    "    frame = frame.reshape((*shape, 1))\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.layers import (Add, Conv2D, Dense, Flatten, Input,\n",
    "                                     Lambda, Subtract)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "#Builds a DQN as a Keras model with 4 2d conolutional networks and relu activation\n",
    "#Arguments:\n",
    "#   n_actions: Number of possible action the agent can take\n",
    "#   learning_rate: Learning rate\n",
    "#   input_shape: Shape of the preprocessed frame the model sees\n",
    "#   history_length: Number of historical frames the agent can see\n",
    "#Returns:\n",
    "#    A compiled Keras model\n",
    "def build_q_network(n_actions, learning_rate=0.00001, input_shape=(84, 84), history_length=4):\n",
    "\n",
    "    model_input = Input(shape=(input_shape[0], input_shape[1], history_length))\n",
    "    x = Lambda(lambda layer: layer / 255)(model_input)  # normalize by 255\n",
    "\n",
    "    x = Conv2D(32, (8, 8), strides=4, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=2, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(64, (3, 3), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(1024, (7, 7), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "\n",
    "    # Split into value and advantage streams\n",
    "    val_stream, adv_stream = Lambda(lambda w: tf.split(w, 2, 3))(x)  # custom splitting layer\n",
    "\n",
    "    val_stream = Flatten()(val_stream)\n",
    "    val = Dense(1, kernel_initializer=VarianceScaling(scale=2.))(val_stream)\n",
    "\n",
    "    adv_stream = Flatten()(adv_stream)\n",
    "    adv = Dense(n_actions, kernel_initializer=VarianceScaling(scale=2.))(adv_stream)\n",
    "        # Combine streams into Q-Values\n",
    "    reduce_mean = Lambda(lambda w: tf.reduce_mean(w, axis=1, keepdims=True))  # custom layer for reduce mean\n",
    "\n",
    "    q_vals = Add()([val, Subtract()([adv, reduce_mean(adv)])])\n",
    "\n",
    "    # Build model\n",
    "    model = Model(model_input, q_vals)\n",
    "    model.compile(Adam(learning_rate), loss=tf.keras.losses.Huber())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "\n",
    "#Wrapper for the environment provided by Gym\n",
    "class GameWrapper:\n",
    "    \n",
    "    def __init__(self, env_name, no_op_steps=10, history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.history_length = 4\n",
    "\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "    \n",
    "    #Resets the environment\n",
    "    #Arguments: \n",
    "    #    evaluation: Set to True when the agent is being evaluated. Takes a random number of no-op steps if True.\n",
    "    def reset(self, evaluation=False): \n",
    "        self.frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "\n",
    "        # If evaluating, take a random number of no-op steps.\n",
    "        # This adds an element of randomness, so that the each evaluation is slightly different.\n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(0, self.no_op_steps)):\n",
    "                self.env.step(1)\n",
    "\n",
    "        # For the initial state, we stack the first frame four times\n",
    "        self.state = np.repeat(process_frame(self.frame), self.history_length, axis=2)\n",
    "    \n",
    "    #performs an action and observes the result\n",
    "    #Arguments:\n",
    "    #    action: An integer describe action the agent chose\n",
    "    #    render_mode: None doesn't render anything, 'human' renders the screen in a new window, \n",
    "    #                 'rgb_array' returns an np.array with rgb values\n",
    "    #Returns:\n",
    "    #    processed_frame: The processed new frame as a result of that action\n",
    "    #    reward: The reward for taking that action\n",
    "    #    terminal: Whether the game has ended\n",
    "    #    life_lost: Whether a life has been lost\n",
    "    #    new_frame: The raw new frame as a result of that action\n",
    "    #    If render_mode is set to 'rgb_array' this also returns the rendered rgb_array\n",
    "    def step(self, action, render_mode=None):\n",
    "\n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "\n",
    "        # In the commonly ignored 'info' or 'meta' data returned by env.step\n",
    "        # we can get information such as the number of lives the agent has.\n",
    "\n",
    "        # We use this here to find out when the agent loses a life, and\n",
    "        # if so, we set life_lost to True.\n",
    "\n",
    "        # We use life_lost to force the agent to start the game\n",
    "        # and not sit around doing nothing.\n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            life_lost = True\n",
    "        else:\n",
    "            life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "\n",
    "        processed_frame = process_frame(new_frame)\n",
    "        self.state = np.append(self.state[:, :, 1:], processed_frame, axis=2)\n",
    "\n",
    "        if render_mode == 'rgb_array':\n",
    "            return processed_frame, reward, terminal, life_lost, self.env.render(render_mode)\n",
    "        elif render_mode == 'human':\n",
    "            self.env.render()\n",
    "\n",
    "        return processed_frame, reward, terminal, life_lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Replay Buffer to store transitions.\n",
    "#This implementation was heavily inspired by Fabio M. Graetz's replay buffer\n",
    "#here: https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    #Arguments:\n",
    "        #size: Integer, Number of stored transitions\n",
    "        #input_shape: Shape of the preprocessed frame\n",
    "        #history_length: Integer, Number of frames stacked together to create a state for the agent\n",
    "        #use_per: Use PER instead of classic experience replay\n",
    "    def __init__(self, size=1000000, batch_size=32, input_shape=(84, 84), history_length=4, use_per=True):\n",
    "        self.size = size\n",
    "        self.input_shape = input_shape\n",
    "        self.history_length = history_length\n",
    "        self.count = 0  # total index of memory written to, always less than self.size\n",
    "        self.current = 0  # index to write to\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.input_shape[0], self.input_shape[1]), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        self.priorities = np.zeros(self.size, dtype=np.float32)\n",
    "\n",
    "        self.use_per = use_per\n",
    "\n",
    "    #Saves a transition to the replay buffer\n",
    "    #Arguments:\n",
    "    #    action: An integer between 0 and env.action_space.n - 1 determining the action the agent perfomed       \n",
    "    #    frame: A (84, 84, 1) frame of the game in grayscale\n",
    "    #    reward: A float determining the reward the agend received for performing an action\n",
    "    #    terminal: A bool stating whether the episode terminated\n",
    "    def add_experience(self, action, frame, reward, terminal, clip_reward=True):\n",
    "        \n",
    "        if frame.shape != self.input_shape:\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "\n",
    "        if clip_reward:\n",
    "            reward = np.sign(reward)\n",
    "\n",
    "        # Write memory\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.priorities[self.current] = max(self.priorities.max(), 1)  # make the most recent experience important\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "        \n",
    "    #Returns a minibatch of self.batch_size = 32 transitions\n",
    "    #Arguments:\n",
    "    #   batch_size: How many samples to return\n",
    "    #   priority_scale: How much to weight priorities. 0 = completely random, 1 = completely based on priority\n",
    "    # Returns:\n",
    "    #   A tuple of states, actions, rewards, new_states, and terminals\n",
    "    #   If use_per is True:\n",
    "    #       An array describing the importance of transition. Used for scaling gradient steps.\n",
    "    #       An array of each index that was sampled\n",
    "    def get_minibatch(self, batch_size=32, priority_scale=0.0):\n",
    "\n",
    "        if self.count < self.history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "\n",
    "        # Get sampling probabilities from priority list\n",
    "        if self.use_per:\n",
    "            scaled_priorities = self.priorities[self.history_length:self.count-1] ** priority_scale\n",
    "            sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "\n",
    "        # Get a list of valid indices\n",
    "        indices = []\n",
    "        for i in range(batch_size):\n",
    "            while True:\n",
    "                # Get a random number from history_length to maximum frame written with probabilities based on priority weights\n",
    "                if self.use_per:\n",
    "                    index = np.random.choice(np.arange(self.history_length, self.count-1), p=sample_probabilities)\n",
    "                else:\n",
    "                    index = random.randint(self.history_length, self.count - 1)\n",
    "\n",
    "                # We check that all frames are from same episode with the two following if statements.  If either are True, the index is invalid.\n",
    "                if index >= self.current and index - self.history_length <= self.current:\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            indices.append(index)\n",
    "\n",
    "        # Retrieve states from memory\n",
    "        states = []\n",
    "        new_states = []\n",
    "        for idx in indices:\n",
    "            states.append(self.frames[idx-self.history_length:idx, ...])\n",
    "            new_states.append(self.frames[idx-self.history_length+1:idx+1, ...])\n",
    "\n",
    "        states = np.transpose(np.asarray(states), axes=(0, 2, 3, 1))\n",
    "        new_states = np.transpose(np.asarray(new_states), axes=(0, 2, 3, 1))\n",
    "\n",
    "        if self.use_per:\n",
    "            # Get importance weights from probabilities calculated earlier\n",
    "            importance = 1/self.count * 1/sample_probabilities[[index - 4 for index in indices]]\n",
    "            importance = importance / importance.max()\n",
    "\n",
    "            return (states, self.actions[indices], self.rewards[indices], new_states, self.terminal_flags[indices]), importance, indices\n",
    "        else:\n",
    "            return states, self.actions[indices], self.rewards[indices], new_states, self.terminal_flags[indices]\n",
    "        \n",
    "    #Update priorities for PER\n",
    "    #Arguments:\n",
    "    #   indices: Indices to update\n",
    "    #   errors: For each index, the error between the target Q-vals and the predicted Q-vals\n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i, e in zip(indices, errors):\n",
    "            self.priorities[i] = abs(e) + offset\n",
    "            \n",
    "    #Save the replay buffer to a folder\n",
    "    def save(self, folder_name):\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.mkdir(folder_name)\n",
    "\n",
    "        np.save(folder_name + '/actions.npy', self.actions)\n",
    "        np.save(folder_name + '/frames.npy', self.frames)\n",
    "        np.save(folder_name + '/rewards.npy', self.rewards)\n",
    "        np.save(folder_name + '/terminal_flags.npy', self.terminal_flags)\n",
    "\n",
    "    #Loads the replay buffer from a folder\n",
    "    def load(self, folder_name):\n",
    "        self.actions = np.load(folder_name + '/actions.npy')\n",
    "        self.frames = np.load(folder_name + '/frames.npy')\n",
    "        self.rewards = np.load(folder_name + '/rewards.npy')\n",
    "        self.terminal_flags = np.load(folder_name + '/terminal_flags.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#This is the agent that evalutates the curent state to determine what action to take next\n",
    "#Arguments:\n",
    "#   dqn: A DQN (returned by the DQN function) to predict moves\n",
    "#   target_dqn: A DQN (returned by the DQN function) to predict target-q values.  This can be initialized in the same way as the dqn argument\n",
    "#   replay_buffer: A ReplayBuffer object for holding all previous experiences\n",
    "#   n_actions: Number of possible actions for the given environment\n",
    "#   input_shape: Tuple/list describing the shape of the pre-processed environment\n",
    "#   batch_size: Number of samples to draw from the replay memory every updating session\n",
    "#   history_length: Number of historical frames available to the agent\n",
    "#   eps_initial: Initial epsilon value.\n",
    "#   eps_final: The \"half-way\" epsilon value.  The epsilon value decreases more slowly after this\n",
    "#   eps_final_frame: The final epsilon value\n",
    "#   eps_evaluation: The epsilon value used during evaluation\n",
    "#   eps_annealing_frames: Number of frames during which epsilon will be annealed to eps_final, then eps_final_frame\n",
    "#   replay_buffer_start_size: Size of replay buffer before beginning to learn (after this many frames, epsilon is decreased more slowly)\n",
    "#   max_frames: Number of total frames the agent will be trained for\n",
    "#   use_per: Use PER instead of classic experience replay\n",
    "class Agent(object):\n",
    "    #Implements a standard DDDQN agent\n",
    "    def __init__(self,\n",
    "                 dqn,\n",
    "                 target_dqn,\n",
    "                 replay_buffer,\n",
    "                 n_actions,\n",
    "                 input_shape=(84, 84),\n",
    "                 batch_size=32,\n",
    "                 history_length=4,\n",
    "                 eps_initial=1,\n",
    "                 eps_final=0.1,\n",
    "                 eps_final_frame=0.01,\n",
    "                 eps_evaluation=0.0,\n",
    "                 eps_annealing_frames=1000000,\n",
    "                 replay_buffer_start_size=50000,\n",
    "                 max_frames=25000000,\n",
    "                 use_per=True):\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.input_shape = input_shape\n",
    "        self.history_length = history_length\n",
    "\n",
    "        # Memory information\n",
    "        self.replay_buffer_start_size = replay_buffer_start_size\n",
    "        self.max_frames = max_frames\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.use_per = use_per\n",
    "\n",
    "        # Epsilon information\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_final_frame = eps_final_frame\n",
    "        self.eps_evaluation = eps_evaluation\n",
    "        self.eps_annealing_frames = eps_annealing_frames\n",
    "\n",
    "        # Slopes and intercepts for exploration decrease\n",
    "        # (Credit to Fabio M. Graetz for this and calculating epsilon based on frame number)\n",
    "        self.slope = -(self.eps_initial - self.eps_final) / self.eps_annealing_frames\n",
    "        self.intercept = self.eps_initial - self.slope*self.replay_buffer_start_size\n",
    "        self.slope_2 = -(self.eps_final - self.eps_final_frame) / (self.max_frames - self.eps_annealing_frames - self.replay_buffer_start_size)\n",
    "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
    "\n",
    "        # DQN\n",
    "        self.DQN = dqn\n",
    "        self.target_dqn = target_dqn\n",
    "        \n",
    "    #Get the appropriate epsilon value from a given frame number\n",
    "    #Arguments:\n",
    "    #   frame_number: Global frame number (used for epsilon)\n",
    "    #   evaluation: True if the model is evaluating, False otherwise (uses eps_evaluation instead of default epsilon value)\n",
    "    #Returns:\n",
    "    #   The appropriate epsilon value\n",
    "    def calc_epsilon(self, frame_number, evaluation=False):\n",
    "        if evaluation:\n",
    "            return self.eps_evaluation\n",
    "        elif frame_number < self.replay_buffer_start_size:\n",
    "            return self.eps_initial\n",
    "        elif frame_number >= self.replay_buffer_start_size and frame_number < self.replay_buffer_start_size + self.eps_annealing_frames:\n",
    "            return self.slope*frame_number + self.intercept\n",
    "        elif frame_number >= self.replay_buffer_start_size + self.eps_annealing_frames:\n",
    "            return self.slope_2*frame_number + self.intercept_2\n",
    "  \n",
    "    #Query the DQN for an action given a state\n",
    "    #Arguments:\n",
    "    #   frame_number: Global frame number (used for epsilon)\n",
    "    #   state: State to give an action for\n",
    "    #   evaluation: True if the model is evaluating, False otherwise (uses eps_evaluation instead of default epsilon value)\n",
    "    #Returns:\n",
    "    #   An integer as the predicted move\n",
    "    def get_action(self, frame_number, state, evaluation=False):\n",
    "    \n",
    "        # Calculate epsilon based on the frame number\n",
    "        eps = self.calc_epsilon(frame_number, evaluation)\n",
    "\n",
    "        # With chance epsilon, take a random action\n",
    "        if np.random.rand(1) < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "\n",
    "        # Otherwise, query the DQN for an action\n",
    "        q_vals = self.DQN.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))[0]\n",
    "        return q_vals.argmax()\n",
    "      \n",
    "    #  Get the output of a hidden layer inside the model.  This will be/is used for visualizing model\n",
    "    #  Arguments:\n",
    "    #      state: The input to the model to get outputs for hidden layers from\n",
    "    #      layer_names: Names of the layers to get outputs from.  This can be a list of multiple names, or a single name\n",
    "    #      stack_state: Stack `state` four times so the model can take input on a single (84, 84, 1) frame\n",
    "    #  Returns:\n",
    "    #      Outputs to the hidden layers specified, in the order they were specified.\n",
    "    def get_intermediate_representation(self, state, layer_names=None, stack_state=True):\n",
    "\n",
    "        # Prepare list of layers\n",
    "        if isinstance(layer_names, list) or isinstance(layer_names, tuple):\n",
    "            layers = [self.DQN.get_layer(name=layer_name).output for layer_name in layer_names]\n",
    "        else:\n",
    "            layers = self.DQN.get_layer(name=layer_names).output\n",
    "\n",
    "        # Model for getting intermediate output\n",
    "        temp_model = tf.keras.Model(self.DQN.inputs, layers)\n",
    "\n",
    "        # Stack state 4 times\n",
    "        if stack_state:\n",
    "            if len(state.shape) == 2:\n",
    "                state = state[:, :, np.newaxis]\n",
    "            state = np.repeat(state, self.history_length, axis=2)\n",
    "\n",
    "        # Put it all together\n",
    "        return temp_model.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))\n",
    "   \n",
    "    #Update the target Q network\n",
    "    def update_target_network(self):\n",
    "        self.target_dqn.set_weights(self.DQN.get_weights())\n",
    "        \n",
    "    #Wrapper function for adding an experience to the Agent's replay buffer\n",
    "    def add_experience(self, action, frame, reward, terminal, clip_reward=True):\n",
    "        self.replay_buffer.add_experience(action, frame, reward, terminal, clip_reward)\n",
    "        \n",
    "    # Sample a batch and use it to improve the DQN\n",
    "    #Arguments:\n",
    "    #    batch_size: How many samples to draw for an update\n",
    "    #    gamma: Reward discount\n",
    "    #    frame_number: Global frame number (used for calculating importances)\n",
    "    #    priority_scale: How much to weight priorities when sampling the replay buffer. 0 = completely random, 1 = completely based on priority\n",
    "    #Returns:\n",
    "    #    The loss between the predicted and target Q as a float\n",
    "    def learn(self, batch_size, gamma, frame_number, priority_scale=1.0):\n",
    "\n",
    "        if self.use_per:\n",
    "            (states, actions, rewards, new_states, terminal_flags), importance, indices = self.replay_buffer.get_minibatch(batch_size=self.batch_size, priority_scale=priority_scale)\n",
    "            importance = importance ** (1-self.calc_epsilon(frame_number))\n",
    "        else:\n",
    "            states, actions, rewards, new_states, terminal_flags = self.replay_buffer.get_minibatch(batch_size=self.batch_size, priority_scale=priority_scale)\n",
    "\n",
    "        # Main DQN estimates best action in new states\n",
    "        arg_q_max = self.DQN.predict(new_states).argmax(axis=1)\n",
    "\n",
    "        # Target DQN estimates q-vals for new states\n",
    "        future_q_vals = self.target_dqn.predict(new_states)\n",
    "        double_q = future_q_vals[range(batch_size), arg_q_max]\n",
    "\n",
    "        # Calculate targets (bellman equation)\n",
    "        target_q = rewards + (gamma*double_q * (1-terminal_flags))\n",
    "\n",
    "        # Use targets to calculate loss (and use loss to calculate gradients)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.DQN(states)\n",
    "\n",
    "            one_hot_actions = tf.keras.utils.to_categorical(actions, self.n_actions, dtype=np.float32)  # using tf.one_hot causes strange errors\n",
    "            Q = tf.reduce_sum(tf.multiply(q_values, one_hot_actions), axis=1)\n",
    "\n",
    "            error = Q - target_q\n",
    "            loss = tf.keras.losses.Huber()(target_q, Q)\n",
    "\n",
    "            if self.use_per:\n",
    "                # Multiply the loss by importance, so that the gradient is also scaled.\n",
    "                # The importance scale reduces bias against situataions that are sampled\n",
    "                # more frequently.\n",
    "                loss = tf.reduce_mean(loss * importance)\n",
    "\n",
    "        model_gradients = tape.gradient(loss, self.DQN.trainable_variables)\n",
    "        self.DQN.optimizer.apply_gradients(zip(model_gradients, self.DQN.trainable_variables))\n",
    "\n",
    "        if self.use_per:\n",
    "            self.replay_buffer.set_priorities(indices, error)\n",
    "\n",
    "        return float(loss.numpy()), error\n",
    "    \n",
    "    #Saves the Agent and all corresponding properties into a folder for later visualizations\n",
    "    #   Arguments:\n",
    "    #       folder_name: Folder in which to save the Agent\n",
    "    #       **kwargs: Agent.save will also save any keyword arguments passed.  \n",
    "    #                  This is used for saving the frame_number\n",
    "    def save(self, folder_name, **kwargs):\n",
    "       \n",
    "        # Create the folder for saving the agent\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        # Save DQN and target DQN\n",
    "        self.DQN.save(folder_name + '/dqn.h5')\n",
    "        self.target_dqn.save(folder_name + '/target_dqn.h5')\n",
    "\n",
    "        # Save replay buffer\n",
    "        self.replay_buffer.save(folder_name + '/replay-buffer')\n",
    "\n",
    "        # Save meta\n",
    "        with open(folder_name + '/meta.json', 'w+') as f:\n",
    "            f.write(json.dumps({**{'buff_count': self.replay_buffer.count, 'buff_curr': self.replay_buffer.current}, **kwargs}))  # save replay_buffer information and any other information\n",
    "        \n",
    "    #Load a previously saved Agent from a folder\n",
    "    #Arguments:\n",
    "    #    folder_name: Folder from which to load the Agent\n",
    "    #Returns:\n",
    "    #    All other saved attributes, e.g., frame number\n",
    "    def load(self, folder_name, load_replay_buffer=True):\n",
    "\n",
    "        if not os.path.isdir(folder_name):\n",
    "            raise ValueError(f'{folder_name} is not a valid directory')\n",
    "\n",
    "        # Load DQNs\n",
    "        self.DQN = tf.keras.models.load_model(folder_name + '/dqn.h5')\n",
    "        self.target_dqn = tf.keras.models.load_model(folder_name + '/target_dqn.h5')\n",
    "        self.optimizer = self.DQN.optimizer\n",
    "\n",
    "        # Load replay buffer\n",
    "        if load_replay_buffer:\n",
    "            self.replay_buffer.load(folder_name + '/replay-buffer')\n",
    "\n",
    "        # Load meta\n",
    "        with open(folder_name + '/meta.json', 'r') as f:\n",
    "            meta = json.load(f)\n",
    "\n",
    "        if load_replay_buffer:\n",
    "            self.replay_buffer.count = meta['buff_count']\n",
    "            self.replay_buffer.current = meta['buff_curr']\n",
    "\n",
    "        del meta['buff_count'], meta['buff_curr']  # we don't want to return this information\n",
    "        return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the Gym environment for the agent to learn & play\n",
    "ENV_NAME = 'MsPacman-v4'\n",
    "\n",
    "# Loading and saving information.\n",
    "# If LOAD_FROM is None, it will train a new agent.\n",
    "# If SAVE_PATH is None, it will not save the agent\n",
    "LOAD_FROM = None\n",
    "SAVE_PATH = 'PacMan-saves'\n",
    "LOAD_REPLAY_BUFFER = True\n",
    "\n",
    "WRITE_TENSORBOARD = True\n",
    "TENSORBOARD_DIR = 'tensorboard/'\n",
    "\n",
    "# If True, use the prioritized experience replay algorithm, instead of regular experience replay\n",
    "# This is much more computationally expensive, but will also allow for better results. Implementing\n",
    "# a binary heap, as recommended in the PER paper, would make this less expensive.\n",
    "# Since Breakout is a simple game, I wouldn't recommend using it here.\n",
    "USE_PER = False\n",
    "\n",
    "PRIORITY_SCALE = 0.7              # How much the replay buffer should sample based on priorities. 0 = complete random samples, 1 = completely aligned with priorities\n",
    "CLIP_REWARD = True                # Any positive reward is +1, and negative reward is -1, 0 is unchanged\n",
    "\n",
    "\n",
    "TOTAL_FRAMES = 30000000           # Total number of frames to train for\n",
    "MAX_EPISODE_LENGTH = 18000        # Maximum length of an episode (in frames).  18000 frames / 60 fps = 5 minutes\n",
    "FRAMES_BETWEEN_EVAL = 100000      # Number of frames between evaluations\n",
    "EVAL_LENGTH = 10000               # Number of frames to evaluate for\n",
    "UPDATE_FREQ = 10000               # Number of actions chosen between updating the target network\n",
    "\n",
    "DISCOUNT_FACTOR = 0.99            # Gamma, how much to discount future rewards\n",
    "MIN_REPLAY_BUFFER_SIZE = 50000    # The minimum size the replay buffer must be before we start to update the agent\n",
    "MEM_SIZE = 1000000                # The maximum size of the replay buffer\n",
    "\n",
    "MAX_NOOP_STEPS = 20               # Randomly perform this number of actions before every evaluation to give it an element of randomness\n",
    "UPDATE_FREQ = 4                   # Number of actions between gradient descent steps\n",
    "\n",
    "INPUT_SHAPE = (84, 84)            # Size of the preprocessed input frame. With the current model architecture, anything below ~80 won't work.\n",
    "BATCH_SIZE = 32                   # Number of samples the agent learns from at once\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 9 actions: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "from tensorflow import summary\n",
    "\n",
    "game_wrapper = GameWrapper(ENV_NAME, MAX_NOOP_STEPS)\n",
    "print(\"The environment has the following {} actions: {}\".format(game_wrapper.env.action_space.n, game_wrapper.env.unwrapped.get_action_meanings()))\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = tf.summary.create_file_writer(TENSORBOARD_DIR)\n",
    "\n",
    "# Build main and target networks\n",
    "MAIN_DQN = build_q_network(game_wrapper.env.action_space.n, LEARNING_RATE, input_shape=INPUT_SHAPE)\n",
    "TARGET_DQN = build_q_network(game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE)\n",
    "\n",
    "replay_buffer = ReplayBuffer(size=MEM_SIZE, input_shape=INPUT_SHAPE, use_per=USE_PER)\n",
    "agent = Agent(MAIN_DQN, TARGET_DQN, replay_buffer, game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE, batch_size=BATCH_SIZE, use_per=USE_PER)\n",
    "\n",
    "# Training and evaluation\n",
    "if LOAD_FROM is None:\n",
    "    frame_number = 0\n",
    "    rewards = []\n",
    "    loss_list = []\n",
    "else:\n",
    "    print('Loading from', LOAD_FROM)\n",
    "    meta = agent.load(LOAD_FROM, LOAD_REPLAY_BUFFER)\n",
    "\n",
    "    # Apply information loaded from meta\n",
    "    frame_number = meta['frame_number']\n",
    "    rewards = meta['rewards']\n",
    "    loss_list = meta['loss_list']\n",
    "\n",
    "    print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexis\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Alexis\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number: 000010  Frame number: 00006448  Average reward: 206.0  Time taken: 1.6s\n",
      "Game number: 000020  Frame number: 00013012  Average reward: 197.0  Time taken: 1.5s\n",
      "Game number: 000030  Frame number: 00019376  Average reward: 223.0  Time taken: 1.5s\n",
      "Game number: 000040  Frame number: 00025409  Average reward: 206.0  Time taken: 1.5s\n",
      "Game number: 000050  Frame number: 00032235  Average reward: 250.0  Time taken: 1.9s\n",
      "Game number: 000060  Frame number: 00039119  Average reward: 192.0  Time taken: 1.9s\n",
      "Game number: 000070  Frame number: 00045714  Average reward: 259.0  Time taken: 1.4s\n",
      "Game number: 000080  Frame number: 00052112  Average reward: 227.0  Time taken: 98.3s\n",
      "Game number: 000090  Frame number: 00058803  Average reward: 221.0  Time taken: 85.5s\n",
      "Game number: 000100  Frame number: 00064988  Average reward: 204.0  Time taken: 69.9s\n",
      "Game number: 000110  Frame number: 00071113  Average reward: 202.0  Time taken: 75.6s\n",
      "Game number: 000120  Frame number: 00077483  Average reward: 203.0  Time taken: 72.9s\n",
      "Game number: 000130  Frame number: 00083759  Average reward: 182.0  Time taken: 88.6s\n",
      "Game number: 000140  Frame number: 00090502  Average reward: 226.0  Time taken: 85.1s\n",
      "Game number: 000150  Frame number: 00097032  Average reward: 241.0  Time taken: 75.5s\n",
      "Evaluation score: 296.0\n",
      "Game number: 000160  Frame number: 00104442  Average reward: 323.0  Time taken: 84.9s\n",
      "Game number: 000170  Frame number: 00110919  Average reward: 219.0  Time taken: 76.8s\n",
      "Game number: 000180  Frame number: 00117070  Average reward: 228.0  Time taken: 54.9s\n",
      "Game number: 000190  Frame number: 00123791  Average reward: 257.0  Time taken: 120.7s\n",
      "Game number: 000200  Frame number: 00130935  Average reward: 307.0  Time taken: 86.6s\n",
      "Game number: 000210  Frame number: 00137203  Average reward: 221.0  Time taken: 62.4s\n",
      "Game number: 000220  Frame number: 00143550  Average reward: 208.0  Time taken: 80.6s\n",
      "Game number: 000230  Frame number: 00149839  Average reward: 226.0  Time taken: 72.9s\n",
      "Game number: 000240  Frame number: 00156700  Average reward: 311.0  Time taken: 85.3s\n",
      "Game number: 000250  Frame number: 00163125  Average reward: 230.0  Time taken: 103.0s\n",
      "Game number: 000260  Frame number: 00169341  Average reward: 227.0  Time taken: 86.2s\n",
      "Game number: 000270  Frame number: 00176643  Average reward: 269.0  Time taken: 67.1s\n",
      "Game number: 000280  Frame number: 00183210  Average reward: 252.0  Time taken: 91.6s\n",
      "Game number: 000290  Frame number: 00189920  Average reward: 278.0  Time taken: 111.9s\n",
      "Game number: 000300  Frame number: 00196167  Average reward: 232.0  Time taken: 86.7s\n",
      "Evaluation score: 346.15384615384613\n",
      "Game number: 000310  Frame number: 00202643  Average reward: 246.0  Time taken: 82.8s\n",
      "Game number: 000320  Frame number: 00209234  Average reward: 251.0  Time taken: 89.7s\n",
      "Game number: 000330  Frame number: 00215551  Average reward: 295.0  Time taken: 79.0s\n",
      "Game number: 000340  Frame number: 00222278  Average reward: 269.0  Time taken: 87.7s\n",
      "Game number: 000350  Frame number: 00230162  Average reward: 360.0  Time taken: 136.4s\n",
      "Game number: 000360  Frame number: 00237075  Average reward: 262.0  Time taken: 69.2s\n",
      "Game number: 000370  Frame number: 00244032  Average reward: 277.0  Time taken: 61.9s\n",
      "Game number: 000380  Frame number: 00250915  Average reward: 281.0  Time taken: 73.9s\n",
      "Game number: 000390  Frame number: 00257628  Average reward: 279.0  Time taken: 78.8s\n",
      "Game number: 000400  Frame number: 00264050  Average reward: 265.0  Time taken: 90.3s\n",
      "Game number: 000410  Frame number: 00271470  Average reward: 405.0  Time taken: 77.6s\n",
      "Game number: 000420  Frame number: 00277680  Average reward: 239.0  Time taken: 90.0s\n",
      "Game number: 000430  Frame number: 00285223  Average reward: 358.0  Time taken: 105.9s\n",
      "Game number: 000440  Frame number: 00292262  Average reward: 327.0  Time taken: 141.1s\n",
      "Game number: 000450  Frame number: 00299687  Average reward: 480.0  Time taken: 62.6s\n",
      "Evaluation score: 418.57142857142856\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "-1533934592 requested and 0 written",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-affd14628c85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# Save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m300\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mSAVE_PATH\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{SAVE_PATH}/save-{str(frame_number).zfill(8)}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_number\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nTraining exited early.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-9fc2db334553>\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, folder_name, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# Save replay buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/replay-buffer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;31m# Save meta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-30d3d6a9359f>\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, folder_name)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/actions.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/frames.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/rewards.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/terminal_flags.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterminal_flags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m--> 553\u001b[1;33m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    554\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[1;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             for chunk in numpy.nditer(\n",
      "\u001b[1;31mOSError\u001b[0m: -1533934592 requested and 0 written"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "import time\n",
    "try:\n",
    "    with writer.as_default():\n",
    "        while frame_number < TOTAL_FRAMES:\n",
    "            # Training\n",
    "\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < FRAMES_BETWEEN_EVAL:\n",
    "                start_time = time.time()\n",
    "                game_wrapper.reset()\n",
    "                life_lost = True\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    # Get action\n",
    "                    action = agent.get_action(frame_number, game_wrapper.state)\n",
    "\n",
    "                    # Take step\n",
    "                    processed_frame, reward, terminal, life_lost = game_wrapper.step(action)\n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "\n",
    "                    # Add experience to replay memory\n",
    "                    agent.add_experience(action=action,\n",
    "                                         frame=processed_frame[:, :, 0],\n",
    "                                         reward=reward, clip_reward=CLIP_REWARD,\n",
    "                                         terminal=life_lost)\n",
    "\n",
    "                    # Update agent\n",
    "                    if frame_number % UPDATE_FREQ == 0 and agent.replay_buffer.count > MIN_REPLAY_BUFFER_SIZE:\n",
    "                        loss, _ = agent.learn(BATCH_SIZE, gamma=DISCOUNT_FACTOR, frame_number=frame_number, priority_scale=PRIORITY_SCALE)\n",
    "                        loss_list.append(loss)\n",
    "\n",
    "                    # Update target network\n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > MIN_REPLAY_BUFFER_SIZE:\n",
    "                        agent.update_target_network()\n",
    "\n",
    "                    # Break the loop when the game is over\n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "\n",
    "                # Output the progress every 10 games\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # Write to TensorBoard\n",
    "                    if WRITE_TENSORBOARD:\n",
    "                        tf.summary.scalar('Reward', np.mean(rewards[-10:]), frame_number)\n",
    "                        tf.summary.scalar('Loss', np.mean(loss_list[-100:]), frame_number)\n",
    "                        writer.flush()\n",
    "\n",
    "                    print(f'Game number: {str(len(rewards)).zfill(6)}  Frame number: {str(frame_number).zfill(8)}  Average reward: {np.mean(rewards[-10:]):0.1f}  Time taken: {(time.time() - start_time):.1f}s')\n",
    "\n",
    "            # Evaluation every `FRAMES_BETWEEN_EVAL` frames\n",
    "            terminal = True\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "\n",
    "            for _ in range(EVAL_LENGTH):\n",
    "                if terminal:\n",
    "                    game_wrapper.reset(evaluation=True)\n",
    "                    life_lost = True\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "\n",
    "                # Breakout requires a \"fire\" action (action #1) to start the\n",
    "                # game each time a life is lost.\n",
    "                # Otherwise, the agent would sit around doing nothing.\n",
    "                action = 1 if life_lost else agent.get_action(frame_number, game_wrapper.state, evaluation=True)\n",
    "\n",
    "                # Step action\n",
    "                _, reward, terminal, life_lost = game_wrapper.step(action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "\n",
    "                # On game-over\n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "\n",
    "            if len(eval_rewards) > 0:\n",
    "                final_score = np.mean(eval_rewards)\n",
    "            else:\n",
    "                # In case the game is longer than the number of frames allowed\n",
    "                final_score = episode_reward_sum\n",
    "            # Print score and write to tensorboard\n",
    "            print('Evaluation score:', final_score)\n",
    "            if WRITE_TENSORBOARD:\n",
    "                tf.summary.scalar('Evaluation score', final_score, frame_number)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model\n",
    "            if len(rewards) > 300 and SAVE_PATH is not None:\n",
    "                agent.save(f'{SAVE_PATH}/save-{str(frame_number).zfill(8)}', frame_number=frame_number, rewards=rewards, loss_list=loss_list)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\nTraining exited early.')\n",
    "    writer.close()\n",
    "\n",
    "    if SAVE_PATH is None:\n",
    "        try:\n",
    "            SAVE_PATH = input('Would you like to save the trained model? If so, type in a save path, otherwise, interrupt with ctrl+c. ')\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nExiting...')\n",
    "\n",
    "    if SAVE_PATH is not None:\n",
    "        print('Saving...')\n",
    "        agent.save(f'{SAVE_PATH}/save-{str(frame_number).zfill(8)}', frame_number=frame_number, rewards=rewards, loss_list=loss_list)\n",
    "        print('Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 9 actions: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']\n",
      "Loading model...\n",
      "Loaded\n",
      "Game over, reward: 280.0, frame: 627/10000\n",
      "Game over, reward: 260.0, frame: 1293/10000\n",
      "Game over, reward: 320.0, frame: 1921/10000\n",
      "Game over, reward: 170.0, frame: 2572/10000\n",
      "Game over, reward: 210.0, frame: 3143/10000\n",
      "Game over, reward: 170.0, frame: 3894/10000\n",
      "Game over, reward: 290.0, frame: 4627/10000\n",
      "Game over, reward: 260.0, frame: 5347/10000\n",
      "Game over, reward: 460.0, frame: 6296/10000\n",
      "Game over, reward: 290.0, frame: 7063/10000\n",
      "Game over, reward: 280.0, frame: 8054/10000\n",
      "Game over, reward: 290.0, frame: 8806/10000\n",
      "Game over, reward: 320.0, frame: 9579/10000\n",
      "Average reward: 276.9230769230769\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'MsPacman-v4'\n",
    "\n",
    "# Create environment\n",
    "game_wrapper = GameWrapper(ENV_NAME, MAX_NOOP_STEPS)\n",
    "print(\"The environment has the following {} actions: {}\".format(game_wrapper.env.action_space.n, game_wrapper.env.unwrapped.get_action_meanings()))\n",
    "\n",
    "# Create agent\n",
    "MAIN_DQN = build_q_network(game_wrapper.env.action_space.n, LEARNING_RATE, input_shape=INPUT_SHAPE)\n",
    "TARGET_DQN = build_q_network(game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE)\n",
    "\n",
    "replay_buffer = ReplayBuffer(size=MEM_SIZE, batch_size=BATCH_SIZE, input_shape=INPUT_SHAPE)\n",
    "agent = Agent(MAIN_DQN, TARGET_DQN, replay_buffer, game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE)\n",
    "\n",
    "print('Loading model...')\n",
    "agent.load('./PacMan-saves/save-00200894')\n",
    "print('Loaded')\n",
    "\n",
    "terminal = True\n",
    "eval_rewards = []\n",
    "evaluate_frame_number = 0\n",
    "\n",
    "for frame in range(EVAL_LENGTH):\n",
    "    if terminal:\n",
    "        game_wrapper.reset(evaluation=True)\n",
    "        life_lost = True\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "\n",
    "    # Breakout require a \"fire\" action (action #1) to start the\n",
    "    # game each time a life is lost.\n",
    "    # Otherwise, the agent would sit around doing nothing.\n",
    "    action = 1 if life_lost else agent.get_action(0, game_wrapper.state, evaluation=True)\n",
    "\n",
    "    # Step action\n",
    "    _, reward, terminal, life_lost = game_wrapper.step(action, render_mode='human')\n",
    "    evaluate_frame_number += 1\n",
    "    episode_reward_sum += reward\n",
    "\n",
    "    # On game-over\n",
    "    if terminal:\n",
    "        print(f'Game over, reward: {episode_reward_sum}, frame: {frame}/{EVAL_LENGTH}')\n",
    "        eval_rewards.append(episode_reward_sum)\n",
    "\n",
    "print('Average reward:', np.mean(eval_rewards) if len(eval_rewards) > 0 else episode_reward_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
