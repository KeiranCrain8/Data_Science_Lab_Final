{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# This function can resize to any shape, but was built to resize to 84x84\n",
    "def process_frame(frame, shape=(84, 84)):\n",
    "    \"\"\"Preprocesses a 210x160x3 frame to 84x84x1 grayscale\n",
    "    Arguments:\n",
    "        frame: The frame to process.  Must have values ranging from 0-255\n",
    "    Returns:\n",
    "        The processed frame\n",
    "    \"\"\"\n",
    "    frame = frame.astype(np.uint8)  # cv2 requires np.uint8, other dtypes will not work\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = frame[34:34+160, :160]  # crop image\n",
    "    frame = cv2.resize(frame, shape, interpolation=cv2.INTER_NEAREST)\n",
    "    frame = frame.reshape((*shape, 1))\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.layers import (Add, Conv2D, Dense, Flatten, Input,\n",
    "                                     Lambda, Subtract)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "def build_q_network(n_actions, learning_rate=0.00001, input_shape=(84, 84), history_length=4):\n",
    "    \"\"\"Builds a dueling DQN as a Keras model\n",
    "    Arguments:\n",
    "        n_actions: Number of possible action the agent can take\n",
    "        learning_rate: Learning rate\n",
    "        input_shape: Shape of the preprocessed frame the model sees\n",
    "        history_length: Number of historical frames the agent can see\n",
    "    Returns:\n",
    "        A compiled Keras model\n",
    "    \"\"\"\n",
    "    model_input = Input(shape=(input_shape[0], input_shape[1], history_length))\n",
    "    x = Lambda(lambda layer: layer / 255)(model_input)  # normalize by 255\n",
    "\n",
    "    x = Conv2D(32, (8, 8), strides=4, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=2, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(64, (3, 3), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(1024, (7, 7), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "\n",
    "    # Split into value and advantage streams\n",
    "    val_stream, adv_stream = Lambda(lambda w: tf.split(w, 2, 3))(x)  # custom splitting layer\n",
    "\n",
    "    val_stream = Flatten()(val_stream)\n",
    "    val = Dense(1, kernel_initializer=VarianceScaling(scale=2.))(val_stream)\n",
    "\n",
    "    adv_stream = Flatten()(adv_stream)\n",
    "    adv = Dense(n_actions, kernel_initializer=VarianceScaling(scale=2.))(adv_stream)\n",
    "        # Combine streams into Q-Values\n",
    "    reduce_mean = Lambda(lambda w: tf.reduce_mean(w, axis=1, keepdims=True))  # custom layer for reduce mean\n",
    "\n",
    "    q_vals = Add()([val, Subtract()([adv, reduce_mean(adv)])])\n",
    "\n",
    "    # Build model\n",
    "    model = Model(model_input, q_vals)\n",
    "    model.compile(Adam(learning_rate), loss=tf.keras.losses.Huber())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# This is the process_frame function we implemented earlier\n",
    "\n",
    "def process_frame(frame, shape=(84, 84)):\n",
    "    \"\"\"Preprocesses a 210x160x3 frame to 84x84x1 grayscale\n",
    "    Arguments:\n",
    "        frame: The frame to process.  Must have values ranging from 0-255\n",
    "    Returns:\n",
    "        The processed frame\n",
    "    \"\"\"\n",
    "    frame = frame.astype(np.uint8)  # cv2 requires np.uint8, other dtypes will not work\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = frame[34:34+160, :160]  # crop image\n",
    "    frame = cv2.resize(frame, shape, interpolation=cv2.INTER_NEAREST)\n",
    "    frame = frame.reshape((*shape, 1))\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "class GameWrapper:\n",
    "    \"\"\"Wrapper for the environment provided by Gym\"\"\"\n",
    "    def __init__(self, env_name, no_op_steps=10, history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.history_length = 4\n",
    "\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "\n",
    "    def reset(self, evaluation=False):\n",
    "        \"\"\"Resets the environment\n",
    "        Arguments:\n",
    "            evaluation: Set to True when the agent is being evaluated. Takes a random number of no-op steps if True.\n",
    "        \"\"\"\n",
    "\n",
    "        self.frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "\n",
    "        # If evaluating, take a random number of no-op steps.\n",
    "        # This adds an element of randomness, so that the each\n",
    "        # evaluation is slightly different.\n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(0, self.no_op_steps)):\n",
    "                self.env.step(1)\n",
    "\n",
    "        # For the initial state, we stack the first frame four times\n",
    "        self.state = np.repeat(process_frame(self.frame), self.history_length, axis=2)\n",
    "\n",
    "    def step(self, action, render_mode=None):\n",
    "        \"\"\"Performs an action and observes the result\n",
    "        Arguments:\n",
    "            action: An integer describe action the agent chose\n",
    "            render_mode: None doesn't render anything, 'human' renders the screen in a new window, 'rgb_array' returns an np.array with rgb values\n",
    "        Returns:\n",
    "            processed_frame: The processed new frame as a result of that action\n",
    "            reward: The reward for taking that action\n",
    "            terminal: Whether the game has ended\n",
    "            life_lost: Whether a life has been lost\n",
    "            new_frame: The raw new frame as a result of that action\n",
    "            If render_mode is set to 'rgb_array' this also returns the rendered rgb_array\n",
    "        \"\"\"\n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "\n",
    "        # In the commonly ignored 'info' or 'meta' data returned by env.step\n",
    "        # we can get information such as the number of lives the agent has.\n",
    "\n",
    "        # We use this here to find out when the agent loses a life, and\n",
    "        # if so, we set life_lost to True.\n",
    "\n",
    "        # We use life_lost to force the agent to start the game\n",
    "        # and not sit around doing nothing.\n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            life_lost = True\n",
    "        else:\n",
    "            life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "\n",
    "        processed_frame = process_frame(new_frame)\n",
    "        self.state = np.append(self.state[:, :, 1:], processed_frame, axis=2)\n",
    "\n",
    "        if render_mode == 'rgb_array':\n",
    "            return processed_frame, reward, terminal, life_lost, self.env.render(render_mode)\n",
    "        elif render_mode == 'human':\n",
    "            self.env.render()\n",
    "\n",
    "        return processed_frame, reward, terminal, life_lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Replay Buffer to store transitions.\n",
    "    This implementation was heavily inspired by Fabio M. Graetz's replay buffer\n",
    "    here: https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb\"\"\"\n",
    "    def __init__(self, size=1000000, batch_size=32, input_shape=(84, 84), history_length=4, use_per=True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            size: Integer, Number of stored transitions\n",
    "            input_shape: Shape of the preprocessed frame\n",
    "            history_length: Integer, Number of frames stacked together to create a state for the agent\n",
    "            use_per: Use PER instead of classic experience replay\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.input_shape = input_shape\n",
    "        self.history_length = history_length\n",
    "        self.count = 0  # total index of memory written to, always less than self.size\n",
    "        self.current = 0  # index to write to\n",
    "        self.batch_size = 32\n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.input_shape[0], self.input_shape[1]), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        self.priorities = np.zeros(self.size, dtype=np.float32)\n",
    "\n",
    "        self.use_per = use_per\n",
    "\n",
    "    def add_experience(self, action, frame, reward, terminal, clip_reward=True):\n",
    "        \"\"\"Saves a transition to the replay buffer\n",
    "        Arguments:\n",
    "            action: An integer between 0 and env.action_space.n - 1 \n",
    "                determining the action the agent perfomed\n",
    "            frame: A (84, 84, 1) frame of the game in grayscale\n",
    "            reward: A float determining the reward the agend received for performing an action\n",
    "            terminal: A bool stating whether the episode terminated\n",
    "        \"\"\"\n",
    "        if frame.shape != self.input_shape:\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "\n",
    "        if clip_reward:\n",
    "            reward = np.sign(reward)\n",
    "\n",
    "        # Write memory\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.priorities[self.current] = max(self.priorities.max(), 1)  # make the most recent experience important\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "\n",
    "    def get_minibatch(self, batch_size=32, priority_scale=0.0):\n",
    "        \"\"\"Returns a minibatch of self.batch_size = 32 transitions\n",
    "        Arguments:\n",
    "            batch_size: How many samples to return\n",
    "            priority_scale: How much to weight priorities. 0 = completely random, 1 = completely based on priority\n",
    "        Returns:\n",
    "            A tuple of states, actions, rewards, new_states, and terminals\n",
    "            If use_per is True:\n",
    "                An array describing the importance of transition. Used for scaling gradient steps.\n",
    "                An array of each index that was sampled\n",
    "        \"\"\"\n",
    "\n",
    "        if self.count < self.history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "\n",
    "        # Get sampling probabilities from priority list\n",
    "        if self.use_per:\n",
    "            scaled_priorities = self.priorities[self.history_length:self.count-1] ** priority_scale\n",
    "            sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "\n",
    "        # Get a list of valid indices\n",
    "        indices = []\n",
    "        for i in range(batch_size):\n",
    "            while True:\n",
    "                # Get a random number from history_length to maximum frame written with probabilities based on priority weights\n",
    "                if self.use_per:\n",
    "                    index = np.random.choice(np.arange(self.history_length, self.count-1), p=sample_probabilities)\n",
    "                else:\n",
    "                    index = random.randint(self.history_length, self.count - 1)\n",
    "\n",
    "                # We check that all frames are from same episode with the two following if statements.  If either are True, the index is invalid.\n",
    "                if index >= self.current and index - self.history_length <= self.current:\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            indices.append(index)\n",
    "\n",
    "        # Retrieve states from memory\n",
    "        states = []\n",
    "        new_states = []\n",
    "        for idx in indices:\n",
    "            states.append(self.frames[idx-self.history_length:idx, ...])\n",
    "            new_states.append(self.frames[idx-self.history_length+1:idx+1, ...])\n",
    "\n",
    "        states = np.transpose(np.asarray(states), axes=(0, 2, 3, 1))\n",
    "        new_states = np.transpose(np.asarray(new_states), axes=(0, 2, 3, 1))\n",
    "\n",
    "        if self.use_per:\n",
    "            # Get importance weights from probabilities calculated earlier\n",
    "            importance = 1/self.count * 1/sample_probabilities[[index - 4 for index in indices]]\n",
    "            importance = importance / importance.max()\n",
    "\n",
    "            return (states, self.actions[indices], self.rewards[indices], new_states, self.terminal_flags[indices]), importance, indices\n",
    "        else:\n",
    "            return states, self.actions[indices], self.rewards[indices], new_states, self.terminal_flags[indices]\n",
    "\n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        \"\"\"Update priorities for PER\n",
    "        Arguments:\n",
    "            indices: Indices to update\n",
    "            errors: For each index, the error between the target Q-vals and the predicted Q-vals\n",
    "        \"\"\"\n",
    "        for i, e in zip(indices, errors):\n",
    "            self.priorities[i] = abs(e) + offset\n",
    "\n",
    "    def save(self, folder_name):\n",
    "        \"\"\"Save the replay buffer to a folder\"\"\"\n",
    "\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.mkdir(folder_name)\n",
    "\n",
    "        np.save(folder_name + '/actions.npy', self.actions)\n",
    "        np.save(folder_name + '/frames.npy', self.frames)\n",
    "        np.save(folder_name + '/rewards.npy', self.rewards)\n",
    "        np.save(folder_name + '/terminal_flags.npy', self.terminal_flags)\n",
    "\n",
    "    def load(self, folder_name):\n",
    "        \"\"\"Loads the replay buffer from a folder\"\"\"\n",
    "        self.actions = np.load(folder_name + '/actions.npy')\n",
    "        self.frames = np.load(folder_name + '/frames.npy')\n",
    "        self.rewards = np.load(folder_name + '/rewards.npy')\n",
    "        self.terminal_flags = np.load(folder_name + '/terminal_flags.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class Agent(object):\n",
    "    \"\"\"Implements a standard DDDQN agent\"\"\"\n",
    "    def __init__(self,\n",
    "                 dqn,\n",
    "                 target_dqn,\n",
    "                 replay_buffer,\n",
    "                 n_actions,\n",
    "                 input_shape=(84, 84),\n",
    "                 batch_size=32,\n",
    "                 history_length=4,\n",
    "                 eps_initial=1,\n",
    "                 eps_final=0.1,\n",
    "                 eps_final_frame=0.01,\n",
    "                 eps_evaluation=0.0,\n",
    "                 eps_annealing_frames=1000000,\n",
    "                 replay_buffer_start_size=50000,\n",
    "                 max_frames=25000000,\n",
    "                 use_per=True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            dqn: A DQN (returned by the DQN function) to predict moves\n",
    "            target_dqn: A DQN (returned by the DQN function) to predict target-q values.  This can be initialized in the same way as the dqn argument\n",
    "            replay_buffer: A ReplayBuffer object for holding all previous experiences\n",
    "            n_actions: Number of possible actions for the given environment\n",
    "            input_shape: Tuple/list describing the shape of the pre-processed environment\n",
    "            batch_size: Number of samples to draw from the replay memory every updating session\n",
    "            history_length: Number of historical frames available to the agent\n",
    "            eps_initial: Initial epsilon value.\n",
    "            eps_final: The \"half-way\" epsilon value.  The epsilon value decreases more slowly after this\n",
    "            eps_final_frame: The final epsilon value\n",
    "            eps_evaluation: The epsilon value used during evaluation\n",
    "            eps_annealing_frames: Number of frames during which epsilon will be annealed to eps_final, then eps_final_frame\n",
    "            replay_buffer_start_size: Size of replay buffer before beginning to learn (after this many frames, epsilon is decreased more slowly)\n",
    "            max_frames: Number of total frames the agent will be trained for\n",
    "            use_per: Use PER instead of classic experience replay\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.input_shape = input_shape\n",
    "        self.history_length = history_length\n",
    "\n",
    "        # Memory information\n",
    "        self.replay_buffer_start_size = replay_buffer_start_size\n",
    "        self.max_frames = max_frames\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.use_per = use_per\n",
    "\n",
    "        # Epsilon information\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_final_frame = eps_final_frame\n",
    "        self.eps_evaluation = eps_evaluation\n",
    "        self.eps_annealing_frames = eps_annealing_frames\n",
    "\n",
    "        # Slopes and intercepts for exploration decrease\n",
    "        # (Credit to Fabio M. Graetz for this and calculating epsilon based on frame number)\n",
    "        self.slope = -(self.eps_initial - self.eps_final) / self.eps_annealing_frames\n",
    "        self.intercept = self.eps_initial - self.slope*self.replay_buffer_start_size\n",
    "        self.slope_2 = -(self.eps_final - self.eps_final_frame) / (self.max_frames - self.eps_annealing_frames - self.replay_buffer_start_size)\n",
    "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
    "\n",
    "        # DQN\n",
    "        self.DQN = dqn\n",
    "        self.target_dqn = target_dqn\n",
    "\n",
    "    def calc_epsilon(self, frame_number, evaluation=False):\n",
    "        \"\"\"Get the appropriate epsilon value from a given frame number\n",
    "        Arguments:\n",
    "            frame_number: Global frame number (used for epsilon)\n",
    "            evaluation: True if the model is evaluating, False otherwise (uses eps_evaluation instead of default epsilon value)\n",
    "        Returns:\n",
    "            The appropriate epsilon value\n",
    "        \"\"\"\n",
    "        if evaluation:\n",
    "            return self.eps_evaluation\n",
    "        elif frame_number < self.replay_buffer_start_size:\n",
    "            return self.eps_initial\n",
    "        elif frame_number >= self.replay_buffer_start_size and frame_number < self.replay_buffer_start_size + self.eps_annealing_frames:\n",
    "            return self.slope*frame_number + self.intercept\n",
    "        elif frame_number >= self.replay_buffer_start_size + self.eps_annealing_frames:\n",
    "            return self.slope_2*frame_number + self.intercept_2\n",
    "\n",
    "    def get_action(self, frame_number, state, evaluation=False):\n",
    "        \"\"\"Query the DQN for an action given a state\n",
    "        Arguments:\n",
    "            frame_number: Global frame number (used for epsilon)\n",
    "            state: State to give an action for\n",
    "            evaluation: True if the model is evaluating, False otherwise (uses eps_evaluation instead of default epsilon value)\n",
    "        Returns:\n",
    "            An integer as the predicted move\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate epsilon based on the frame number\n",
    "        eps = self.calc_epsilon(frame_number, evaluation)\n",
    "\n",
    "        # With chance epsilon, take a random action\n",
    "        if np.random.rand(1) < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "\n",
    "        # Otherwise, query the DQN for an action\n",
    "        q_vals = self.DQN.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))[0]\n",
    "        return q_vals.argmax()\n",
    "\n",
    "    def get_intermediate_representation(self, state, layer_names=None, stack_state=True):\n",
    "        \"\"\"\n",
    "        Get the output of a hidden layer inside the model.  This will be/is used for visualizing model\n",
    "        Arguments:\n",
    "            state: The input to the model to get outputs for hidden layers from\n",
    "            layer_names: Names of the layers to get outputs from.  This can be a list of multiple names, or a single name\n",
    "            stack_state: Stack `state` four times so the model can take input on a single (84, 84, 1) frame\n",
    "        Returns:\n",
    "            Outputs to the hidden layers specified, in the order they were specified.\n",
    "        \"\"\"\n",
    "        # Prepare list of layers\n",
    "        if isinstance(layer_names, list) or isinstance(layer_names, tuple):\n",
    "            layers = [self.DQN.get_layer(name=layer_name).output for layer_name in layer_names]\n",
    "        else:\n",
    "            layers = self.DQN.get_layer(name=layer_names).output\n",
    "\n",
    "        # Model for getting intermediate output\n",
    "        temp_model = tf.keras.Model(self.DQN.inputs, layers)\n",
    "\n",
    "        # Stack state 4 times\n",
    "        if stack_state:\n",
    "            if len(state.shape) == 2:\n",
    "                state = state[:, :, np.newaxis]\n",
    "            state = np.repeat(state, self.history_length, axis=2)\n",
    "\n",
    "        # Put it all together\n",
    "        return temp_model.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update the target Q network\"\"\"\n",
    "        self.target_dqn.set_weights(self.DQN.get_weights())\n",
    "\n",
    "    def add_experience(self, action, frame, reward, terminal, clip_reward=True):\n",
    "        \"\"\"Wrapper function for adding an experience to the Agent's replay buffer\"\"\"\n",
    "        self.replay_buffer.add_experience(action, frame, reward, terminal, clip_reward)\n",
    "\n",
    "    def learn(self, batch_size, gamma, frame_number, priority_scale=1.0):\n",
    "        \"\"\"Sample a batch and use it to improve the DQN\n",
    "        Arguments:\n",
    "            batch_size: How many samples to draw for an update\n",
    "            gamma: Reward discount\n",
    "            frame_number: Global frame number (used for calculating importances)\n",
    "            priority_scale: How much to weight priorities when sampling the replay buffer. 0 = completely random, 1 = completely based on priority\n",
    "        Returns:\n",
    "            The loss between the predicted and target Q as a float\n",
    "        \"\"\"\n",
    "\n",
    "        if self.use_per:\n",
    "            (states, actions, rewards, new_states, terminal_flags), importance, indices = self.replay_buffer.get_minibatch(batch_size=self.batch_size, priority_scale=priority_scale)\n",
    "            importance = importance ** (1-self.calc_epsilon(frame_number))\n",
    "        else:\n",
    "            states, actions, rewards, new_states, terminal_flags = self.replay_buffer.get_minibatch(batch_size=self.batch_size, priority_scale=priority_scale)\n",
    "\n",
    "        # Main DQN estimates best action in new states\n",
    "        arg_q_max = self.DQN.predict(new_states).argmax(axis=1)\n",
    "\n",
    "        # Target DQN estimates q-vals for new states\n",
    "        future_q_vals = self.target_dqn.predict(new_states)\n",
    "        double_q = future_q_vals[range(batch_size), arg_q_max]\n",
    "\n",
    "        # Calculate targets (bellman equation)\n",
    "        target_q = rewards + (gamma*double_q * (1-terminal_flags))\n",
    "\n",
    "        # Use targets to calculate loss (and use loss to calculate gradients)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.DQN(states)\n",
    "\n",
    "            one_hot_actions = tf.keras.utils.to_categorical(actions, self.n_actions, dtype=np.float32)  # using tf.one_hot causes strange errors\n",
    "            Q = tf.reduce_sum(tf.multiply(q_values, one_hot_actions), axis=1)\n",
    "\n",
    "            error = Q - target_q\n",
    "            loss = tf.keras.losses.Huber()(target_q, Q)\n",
    "\n",
    "            if self.use_per:\n",
    "                # Multiply the loss by importance, so that the gradient is also scaled.\n",
    "                # The importance scale reduces bias against situataions that are sampled\n",
    "                # more frequently.\n",
    "                loss = tf.reduce_mean(loss * importance)\n",
    "\n",
    "        model_gradients = tape.gradient(loss, self.DQN.trainable_variables)\n",
    "        self.DQN.optimizer.apply_gradients(zip(model_gradients, self.DQN.trainable_variables))\n",
    "\n",
    "        if self.use_per:\n",
    "            self.replay_buffer.set_priorities(indices, error)\n",
    "\n",
    "        return float(loss.numpy()), error\n",
    "\n",
    "    def save(self, folder_name, **kwargs):\n",
    "        \"\"\"Saves the Agent and all corresponding properties into a folder\n",
    "        Arguments:\n",
    "            folder_name: Folder in which to save the Agent\n",
    "            **kwargs: Agent.save will also save any keyword arguments passed.  This is used for saving the frame_number\n",
    "        \"\"\"\n",
    "\n",
    "        # Create the folder for saving the agent\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        # Save DQN and target DQN\n",
    "        self.DQN.save(folder_name + '/dqn.h5')\n",
    "        self.target_dqn.save(folder_name + '/target_dqn.h5')\n",
    "\n",
    "        # Save replay buffer\n",
    "        self.replay_buffer.save(folder_name + '/replay-buffer')\n",
    "\n",
    "        # Save meta\n",
    "        with open(folder_name + '/meta.json', 'w+') as f:\n",
    "            f.write(json.dumps({**{'buff_count': self.replay_buffer.count, 'buff_curr': self.replay_buffer.current}, **kwargs}))  # save replay_buffer information and any other information\n",
    "\n",
    "    def load(self, folder_name, load_replay_buffer=True):\n",
    "        \"\"\"Load a previously saved Agent from a folder\n",
    "        Arguments:\n",
    "            folder_name: Folder from which to load the Agent\n",
    "        Returns:\n",
    "            All other saved attributes, e.g., frame number\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.isdir(folder_name):\n",
    "            raise ValueError(f'{folder_name} is not a valid directory')\n",
    "\n",
    "        # Load DQNs\n",
    "        self.DQN = tf.keras.models.load_model(folder_name + '/dqn.h5')\n",
    "        self.target_dqn = tf.keras.models.load_model(folder_name + '/target_dqn.h5')\n",
    "        self.optimizer = self.DQN.optimizer\n",
    "\n",
    "        # Load replay buffer\n",
    "        if load_replay_buffer:\n",
    "            self.replay_buffer.load(folder_name + '/replay-buffer')\n",
    "\n",
    "        # Load meta\n",
    "        with open(folder_name + '/meta.json', 'r') as f:\n",
    "            meta = json.load(f)\n",
    "\n",
    "        if load_replay_buffer:\n",
    "            self.replay_buffer.count = meta['buff_count']\n",
    "            self.replay_buffer.current = meta['buff_curr']\n",
    "\n",
    "        del meta['buff_count'], meta['buff_curr']  # we don't want to return this information\n",
    "        return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the Gym environment for the agent to learn & play\n",
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "\n",
    "# Loading and saving information.\n",
    "# If LOAD_FROM is None, it will train a new agent.\n",
    "# If SAVE_PATH is None, it will not save the agent\n",
    "LOAD_FROM = None\n",
    "SAVE_PATH = 'breakout-saves'\n",
    "LOAD_REPLAY_BUFFER = True\n",
    "\n",
    "WRITE_TENSORBOARD = True\n",
    "TENSORBOARD_DIR = 'tensorboard/'\n",
    "\n",
    "# If True, use the prioritized experience replay algorithm, instead of regular experience replay\n",
    "# This is much more computationally expensive, but will also allow for better results. Implementing\n",
    "# a binary heap, as recommended in the PER paper, would make this less expensive.\n",
    "# Since Breakout is a simple game, I wouldn't recommend using it here.\n",
    "USE_PER = False\n",
    "\n",
    "PRIORITY_SCALE = 0.7              # How much the replay buffer should sample based on priorities. 0 = complete random samples, 1 = completely aligned with priorities\n",
    "CLIP_REWARD = True                # Any positive reward is +1, and negative reward is -1, 0 is unchanged\n",
    "\n",
    "\n",
    "TOTAL_FRAMES = 30000000           # Total number of frames to train for\n",
    "MAX_EPISODE_LENGTH = 18000        # Maximum length of an episode (in frames).  18000 frames / 60 fps = 5 minutes\n",
    "FRAMES_BETWEEN_EVAL = 100000      # Number of frames between evaluations\n",
    "EVAL_LENGTH = 10000               # Number of frames to evaluate for\n",
    "UPDATE_FREQ = 10000               # Number of actions chosen between updating the target network\n",
    "\n",
    "DISCOUNT_FACTOR = 0.99            # Gamma, how much to discount future rewards\n",
    "MIN_REPLAY_BUFFER_SIZE = 50000    # The minimum size the replay buffer must be before we start to update the agent\n",
    "MEM_SIZE = 1000000                # The maximum size of the replay buffer\n",
    "\n",
    "MAX_NOOP_STEPS = 20               # Randomly perform this number of actions before every evaluation to give it an element of randomness\n",
    "UPDATE_FREQ = 4                   # Number of actions between gradient descent steps\n",
    "\n",
    "INPUT_SHAPE = (84, 84)            # Size of the preprocessed input frame. With the current model architecture, anything below ~80 won't work.\n",
    "BATCH_SIZE = 32                   # Number of samples the agent learns from at once\n",
    "LEARNING_RATE = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "from tensorflow import summary\n",
    "\n",
    "game_wrapper = GameWrapper(ENV_NAME, MAX_NOOP_STEPS)\n",
    "print(\"The environment has the following {} actions: {}\".format(game_wrapper.env.action_space.n, game_wrapper.env.unwrapped.get_action_meanings()))\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = tf.summary.create_file_writer(TENSORBOARD_DIR)\n",
    "\n",
    "# Build main and target networks\n",
    "MAIN_DQN = build_q_network(game_wrapper.env.action_space.n, LEARNING_RATE, input_shape=INPUT_SHAPE)\n",
    "TARGET_DQN = build_q_network(game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE)\n",
    "\n",
    "replay_buffer = ReplayBuffer(size=MEM_SIZE, input_shape=INPUT_SHAPE, use_per=USE_PER)\n",
    "agent = Agent(MAIN_DQN, TARGET_DQN, replay_buffer, game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE, batch_size=BATCH_SIZE, use_per=USE_PER)\n",
    "\n",
    "# Training and evaluation\n",
    "if LOAD_FROM is None:\n",
    "    frame_number = 0\n",
    "    rewards = []\n",
    "    loss_list = []\n",
    "else:\n",
    "    print('Loading from', LOAD_FROM)\n",
    "    meta = agent.load(LOAD_FROM, LOAD_REPLAY_BUFFER)\n",
    "\n",
    "    # Apply information loaded from meta\n",
    "    frame_number = meta['frame_number']\n",
    "    rewards = meta['rewards']\n",
    "    loss_list = meta['loss_list']\n",
    "\n",
    "    print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tania\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Tania\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number: 000010  Frame number: 00001851  Average reward: 1.3  Time taken: 0.4s\n",
      "Game number: 000020  Frame number: 00003695  Average reward: 1.2  Time taken: 0.6s\n",
      "Game number: 000030  Frame number: 00005434  Average reward: 1.0  Time taken: 0.4s\n",
      "Game number: 000040  Frame number: 00006882  Average reward: 0.2  Time taken: 0.4s\n",
      "Game number: 000050  Frame number: 00008520  Average reward: 0.7  Time taken: 0.4s\n",
      "Game number: 000060  Frame number: 00010377  Average reward: 1.4  Time taken: 0.6s\n",
      "Game number: 000070  Frame number: 00012109  Average reward: 1.0  Time taken: 0.6s\n",
      "Game number: 000080  Frame number: 00013800  Average reward: 0.9  Time taken: 0.4s\n",
      "Game number: 000090  Frame number: 00015489  Average reward: 0.8  Time taken: 0.6s\n",
      "Game number: 000100  Frame number: 00017292  Average reward: 1.1  Time taken: 0.5s\n",
      "Game number: 000110  Frame number: 00018947  Average reward: 0.8  Time taken: 0.5s\n",
      "Game number: 000120  Frame number: 00020928  Average reward: 1.5  Time taken: 0.6s\n",
      "Game number: 000130  Frame number: 00022683  Average reward: 0.9  Time taken: 0.5s\n",
      "Game number: 000140  Frame number: 00024454  Average reward: 1.1  Time taken: 0.5s\n",
      "Game number: 000150  Frame number: 00026354  Average reward: 1.4  Time taken: 0.4s\n",
      "Game number: 000160  Frame number: 00027868  Average reward: 0.4  Time taken: 0.5s\n",
      "Game number: 000170  Frame number: 00029330  Average reward: 0.3  Time taken: 0.4s\n",
      "Game number: 000180  Frame number: 00031237  Average reward: 1.4  Time taken: 0.4s\n",
      "Game number: 000190  Frame number: 00032725  Average reward: 0.3  Time taken: 0.4s\n",
      "Game number: 000200  Frame number: 00034450  Average reward: 0.9  Time taken: 0.7s\n",
      "Game number: 000210  Frame number: 00036308  Average reward: 1.4  Time taken: 0.6s\n",
      "Game number: 000220  Frame number: 00038325  Average reward: 1.8  Time taken: 0.4s\n",
      "Game number: 000230  Frame number: 00040153  Average reward: 1.2  Time taken: 0.5s\n",
      "Game number: 000240  Frame number: 00041870  Average reward: 0.9  Time taken: 0.5s\n",
      "Game number: 000250  Frame number: 00043664  Average reward: 1.1  Time taken: 0.5s\n",
      "Game number: 000260  Frame number: 00045508  Average reward: 1.1  Time taken: 0.4s\n",
      "Game number: 000270  Frame number: 00047407  Average reward: 1.4  Time taken: 1.0s\n",
      "Game number: 000280  Frame number: 00049144  Average reward: 1.0  Time taken: 0.7s\n",
      "Game number: 000290  Frame number: 00051053  Average reward: 1.4  Time taken: 23.4s\n",
      "Game number: 000300  Frame number: 00053128  Average reward: 1.7  Time taken: 33.9s\n",
      "Game number: 000310  Frame number: 00055196  Average reward: 1.9  Time taken: 38.3s\n",
      "Game number: 000320  Frame number: 00056977  Average reward: 1.1  Time taken: 22.2s\n",
      "Game number: 000330  Frame number: 00058868  Average reward: 1.2  Time taken: 25.0s\n",
      "Game number: 000340  Frame number: 00060440  Average reward: 0.7  Time taken: 23.4s\n",
      "Game number: 000350  Frame number: 00062078  Average reward: 0.7  Time taken: 16.2s\n",
      "Game number: 000360  Frame number: 00063820  Average reward: 0.9  Time taken: 27.4s\n",
      "Game number: 000370  Frame number: 00065690  Average reward: 1.4  Time taken: 18.3s\n",
      "Game number: 000380  Frame number: 00067486  Average reward: 1.1  Time taken: 17.2s\n",
      "Game number: 000390  Frame number: 00069238  Average reward: 1.1  Time taken: 17.8s\n",
      "Game number: 000400  Frame number: 00071154  Average reward: 1.5  Time taken: 17.6s\n",
      "Game number: 000410  Frame number: 00073143  Average reward: 1.8  Time taken: 22.0s\n",
      "Game number: 000420  Frame number: 00074665  Average reward: 0.5  Time taken: 18.1s\n",
      "Game number: 000430  Frame number: 00076284  Average reward: 0.6  Time taken: 25.2s\n",
      "Game number: 000440  Frame number: 00077965  Average reward: 0.9  Time taken: 28.1s\n",
      "Game number: 000450  Frame number: 00079903  Average reward: 1.4  Time taken: 25.1s\n",
      "Game number: 000460  Frame number: 00081366  Average reward: 0.3  Time taken: 17.3s\n",
      "Game number: 000470  Frame number: 00083189  Average reward: 1.3  Time taken: 17.2s\n",
      "Game number: 000480  Frame number: 00085022  Average reward: 1.4  Time taken: 20.5s\n",
      "Game number: 000490  Frame number: 00086645  Average reward: 0.7  Time taken: 18.1s\n",
      "Game number: 000500  Frame number: 00088256  Average reward: 0.8  Time taken: 19.3s\n",
      "Game number: 000510  Frame number: 00089917  Average reward: 0.9  Time taken: 18.2s\n",
      "Game number: 000520  Frame number: 00091736  Average reward: 1.3  Time taken: 16.9s\n",
      "Game number: 000530  Frame number: 00093507  Average reward: 1.1  Time taken: 23.7s\n",
      "Game number: 000540  Frame number: 00095762  Average reward: 2.1  Time taken: 53.3s\n",
      "Game number: 000550  Frame number: 00097301  Average reward: 0.6  Time taken: 17.7s\n",
      "Game number: 000560  Frame number: 00098924  Average reward: 0.8  Time taken: 20.1s\n",
      "Evaluation score: 2.0\n",
      "Game number: 000570  Frame number: 00100590  Average reward: 0.8  Time taken: 31.0s\n",
      "Game number: 000580  Frame number: 00102135  Average reward: 0.7  Time taken: 18.9s\n",
      "Game number: 000590  Frame number: 00103812  Average reward: 0.9  Time taken: 18.0s\n",
      "Game number: 000600  Frame number: 00105856  Average reward: 1.8  Time taken: 49.0s\n",
      "Game number: 000610  Frame number: 00107586  Average reward: 1.1  Time taken: 43.2s\n",
      "Game number: 000620  Frame number: 00109362  Average reward: 1.1  Time taken: 21.5s\n",
      "Game number: 000630  Frame number: 00111084  Average reward: 1.2  Time taken: 25.3s\n",
      "Game number: 000640  Frame number: 00112777  Average reward: 1.0  Time taken: 36.2s\n",
      "Game number: 000650  Frame number: 00114760  Average reward: 1.6  Time taken: 31.4s\n",
      "Game number: 000660  Frame number: 00116840  Average reward: 1.9  Time taken: 73.6s\n",
      "Game number: 000670  Frame number: 00118556  Average reward: 1.1  Time taken: 31.4s\n",
      "Game number: 000680  Frame number: 00120197  Average reward: 0.9  Time taken: 36.4s\n",
      "Game number: 000690  Frame number: 00122343  Average reward: 2.1  Time taken: 78.9s\n",
      "Game number: 000700  Frame number: 00124125  Average reward: 1.2  Time taken: 33.8s\n",
      "Game number: 000710  Frame number: 00126119  Average reward: 1.8  Time taken: 54.0s\n",
      "Game number: 000720  Frame number: 00127920  Average reward: 1.3  Time taken: 58.1s\n",
      "Game number: 000730  Frame number: 00129579  Average reward: 0.8  Time taken: 28.0s\n",
      "Game number: 000740  Frame number: 00131544  Average reward: 1.5  Time taken: 39.3s\n",
      "Game number: 000750  Frame number: 00133341  Average reward: 1.2  Time taken: 64.2s\n",
      "Game number: 000760  Frame number: 00134967  Average reward: 0.8  Time taken: 39.2s\n",
      "Game number: 000770  Frame number: 00136942  Average reward: 1.8  Time taken: 55.9s\n",
      "Game number: 000780  Frame number: 00138724  Average reward: 1.4  Time taken: 62.8s\n",
      "Game number: 000790  Frame number: 00140450  Average reward: 1.0  Time taken: 41.1s\n",
      "Game number: 000800  Frame number: 00142422  Average reward: 1.8  Time taken: 43.6s\n",
      "Game number: 000810  Frame number: 00144251  Average reward: 1.5  Time taken: 41.4s\n",
      "Game number: 000820  Frame number: 00146215  Average reward: 1.6  Time taken: 68.4s\n",
      "Game number: 000830  Frame number: 00148278  Average reward: 2.1  Time taken: 21.6s\n",
      "Game number: 000840  Frame number: 00150299  Average reward: 1.9  Time taken: 40.4s\n",
      "Game number: 000850  Frame number: 00152191  Average reward: 1.6  Time taken: 25.2s\n",
      "Game number: 000860  Frame number: 00154043  Average reward: 1.3  Time taken: 23.0s\n",
      "Game number: 000870  Frame number: 00156145  Average reward: 2.3  Time taken: 39.3s\n",
      "Game number: 000880  Frame number: 00158116  Average reward: 1.9  Time taken: 34.2s\n",
      "Game number: 000890  Frame number: 00160095  Average reward: 2.0  Time taken: 50.1s\n",
      "Game number: 000900  Frame number: 00162159  Average reward: 1.8  Time taken: 34.9s\n",
      "Game number: 000910  Frame number: 00164014  Average reward: 1.3  Time taken: 38.4s\n",
      "Game number: 000920  Frame number: 00166032  Average reward: 1.8  Time taken: 31.6s\n",
      "Game number: 000930  Frame number: 00168049  Average reward: 1.9  Time taken: 28.2s\n",
      "Game number: 000940  Frame number: 00169938  Average reward: 1.7  Time taken: 22.1s\n",
      "Game number: 000950  Frame number: 00172203  Average reward: 2.5  Time taken: 51.0s\n",
      "Game number: 000960  Frame number: 00174051  Average reward: 1.5  Time taken: 25.0s\n",
      "Game number: 000970  Frame number: 00176362  Average reward: 2.7  Time taken: 41.7s\n",
      "Game number: 000980  Frame number: 00178459  Average reward: 2.1  Time taken: 26.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number: 000990  Frame number: 00180491  Average reward: 2.0  Time taken: 29.0s\n",
      "Game number: 001000  Frame number: 00182347  Average reward: 1.6  Time taken: 42.6s\n",
      "Game number: 001010  Frame number: 00184296  Average reward: 1.8  Time taken: 29.5s\n",
      "Game number: 001020  Frame number: 00186287  Average reward: 1.9  Time taken: 26.9s\n",
      "Game number: 001030  Frame number: 00188370  Average reward: 2.2  Time taken: 42.5s\n",
      "Game number: 001040  Frame number: 00190157  Average reward: 1.5  Time taken: 26.5s\n",
      "Game number: 001050  Frame number: 00192169  Average reward: 2.0  Time taken: 26.1s\n",
      "Game number: 001060  Frame number: 00194348  Average reward: 2.6  Time taken: 27.4s\n",
      "Game number: 001070  Frame number: 00196319  Average reward: 1.8  Time taken: 25.8s\n",
      "Game number: 001080  Frame number: 00198484  Average reward: 2.4  Time taken: 42.5s\n",
      "Evaluation score: 3.826923076923077\n",
      "Game number: 001090  Frame number: 00200348  Average reward: 1.6  Time taken: 31.3s\n",
      "Game number: 001100  Frame number: 00202116  Average reward: 1.4  Time taken: 48.8s\n",
      "Game number: 001110  Frame number: 00204081  Average reward: 1.8  Time taken: 54.5s\n",
      "Game number: 001120  Frame number: 00206469  Average reward: 3.0  Time taken: 63.3s\n",
      "Game number: 001130  Frame number: 00208654  Average reward: 2.4  Time taken: 47.0s\n",
      "Game number: 001140  Frame number: 00210447  Average reward: 1.5  Time taken: 37.7s\n",
      "Game number: 001150  Frame number: 00212534  Average reward: 2.3  Time taken: 48.3s\n",
      "Game number: 001160  Frame number: 00214403  Average reward: 1.4  Time taken: 47.3s\n",
      "Game number: 001170  Frame number: 00216324  Average reward: 1.8  Time taken: 40.3s\n",
      "Game number: 001180  Frame number: 00218260  Average reward: 1.8  Time taken: 31.9s\n",
      "Game number: 001190  Frame number: 00220357  Average reward: 2.2  Time taken: 33.1s\n",
      "Game number: 001200  Frame number: 00222285  Average reward: 1.8  Time taken: 43.4s\n",
      "Game number: 001210  Frame number: 00224689  Average reward: 2.9  Time taken: 30.5s\n",
      "Game number: 001220  Frame number: 00226948  Average reward: 2.6  Time taken: 32.8s\n",
      "Game number: 001230  Frame number: 00229299  Average reward: 2.8  Time taken: 46.2s\n",
      "Game number: 001240  Frame number: 00231258  Average reward: 1.8  Time taken: 28.0s\n",
      "Game number: 001250  Frame number: 00233408  Average reward: 2.4  Time taken: 57.5s\n",
      "Game number: 001260  Frame number: 00235427  Average reward: 2.0  Time taken: 24.7s\n",
      "Game number: 001270  Frame number: 00237415  Average reward: 2.0  Time taken: 30.4s\n",
      "Game number: 001280  Frame number: 00239520  Average reward: 2.3  Time taken: 24.6s\n",
      "Game number: 001290  Frame number: 00241353  Average reward: 1.7  Time taken: 28.1s\n",
      "Game number: 001300  Frame number: 00243364  Average reward: 2.1  Time taken: 36.4s\n",
      "Game number: 001310  Frame number: 00245619  Average reward: 2.4  Time taken: 43.3s\n",
      "Game number: 001320  Frame number: 00247775  Average reward: 2.4  Time taken: 32.3s\n",
      "Game number: 001330  Frame number: 00249565  Average reward: 1.5  Time taken: 25.1s\n",
      "Game number: 001340  Frame number: 00251655  Average reward: 2.2  Time taken: 37.5s\n",
      "Game number: 001350  Frame number: 00253706  Average reward: 2.1  Time taken: 38.8s\n",
      "Game number: 001360  Frame number: 00255934  Average reward: 2.9  Time taken: 24.0s\n",
      "Game number: 001370  Frame number: 00258164  Average reward: 2.6  Time taken: 24.3s\n",
      "Game number: 001380  Frame number: 00260117  Average reward: 2.0  Time taken: 37.1s\n",
      "Game number: 001390  Frame number: 00261876  Average reward: 1.4  Time taken: 31.9s\n",
      "Game number: 001400  Frame number: 00263725  Average reward: 1.6  Time taken: 24.5s\n",
      "Game number: 001410  Frame number: 00265724  Average reward: 1.9  Time taken: 24.0s\n",
      "Game number: 001420  Frame number: 00267991  Average reward: 2.6  Time taken: 25.1s\n",
      "Game number: 001430  Frame number: 00270242  Average reward: 2.5  Time taken: 36.5s\n",
      "Game number: 001440  Frame number: 00272463  Average reward: 3.0  Time taken: 36.6s\n",
      "Game number: 001450  Frame number: 00274493  Average reward: 2.2  Time taken: 31.9s\n",
      "Game number: 001460  Frame number: 00276424  Average reward: 2.0  Time taken: 24.7s\n",
      "Game number: 001470  Frame number: 00278636  Average reward: 2.5  Time taken: 49.3s\n",
      "Game number: 001480  Frame number: 00280512  Average reward: 1.8  Time taken: 29.8s\n",
      "Game number: 001490  Frame number: 00282392  Average reward: 1.9  Time taken: 32.1s\n",
      "Game number: 001500  Frame number: 00284460  Average reward: 2.3  Time taken: 44.5s\n",
      "Game number: 001510  Frame number: 00287051  Average reward: 3.5  Time taken: 29.9s\n",
      "Game number: 001520  Frame number: 00288936  Average reward: 2.0  Time taken: 34.6s\n",
      "Game number: 001530  Frame number: 00290567  Average reward: 1.1  Time taken: 28.3s\n",
      "Game number: 001540  Frame number: 00292679  Average reward: 2.2  Time taken: 40.0s\n",
      "Game number: 001550  Frame number: 00294579  Average reward: 1.8  Time taken: 24.4s\n",
      "Game number: 001560  Frame number: 00296477  Average reward: 1.8  Time taken: 21.9s\n",
      "Game number: 001570  Frame number: 00299003  Average reward: 3.5  Time taken: 38.0s\n",
      "Evaluation score: 2.309090909090909\n",
      "Game number: 001580  Frame number: 00301109  Average reward: 2.3  Time taken: 28.7s\n",
      "Game number: 001590  Frame number: 00303311  Average reward: 2.6  Time taken: 43.9s\n",
      "Game number: 001600  Frame number: 00305533  Average reward: 2.5  Time taken: 22.7s\n",
      "Game number: 001610  Frame number: 00307632  Average reward: 2.9  Time taken: 35.5s\n",
      "Game number: 001620  Frame number: 00309785  Average reward: 2.8  Time taken: 48.6s\n",
      "Game number: 001630  Frame number: 00311776  Average reward: 2.1  Time taken: 51.0s\n",
      "Game number: 001640  Frame number: 00314093  Average reward: 2.8  Time taken: 32.8s\n",
      "Game number: 001650  Frame number: 00316405  Average reward: 2.8  Time taken: 34.8s\n",
      "Game number: 001660  Frame number: 00318619  Average reward: 3.0  Time taken: 21.4s\n",
      "Game number: 001670  Frame number: 00320504  Average reward: 1.8  Time taken: 25.4s\n",
      "Game number: 001680  Frame number: 00322886  Average reward: 3.2  Time taken: 27.5s\n",
      "Game number: 001690  Frame number: 00325126  Average reward: 2.8  Time taken: 20.8s\n",
      "Game number: 001700  Frame number: 00327426  Average reward: 2.7  Time taken: 20.1s\n",
      "Game number: 001710  Frame number: 00329709  Average reward: 3.3  Time taken: 22.2s\n",
      "Game number: 001720  Frame number: 00331624  Average reward: 1.7  Time taken: 21.2s\n",
      "Game number: 001730  Frame number: 00333891  Average reward: 2.8  Time taken: 21.4s\n",
      "Game number: 001740  Frame number: 00335939  Average reward: 2.8  Time taken: 22.3s\n",
      "Game number: 001750  Frame number: 00338693  Average reward: 5.3  Time taken: 37.6s\n",
      "Game number: 001760  Frame number: 00340887  Average reward: 2.5  Time taken: 61.4s\n",
      "Game number: 001770  Frame number: 00343036  Average reward: 2.5  Time taken: 17.8s\n",
      "Game number: 001780  Frame number: 00345353  Average reward: 3.4  Time taken: 31.4s\n",
      "Game number: 001790  Frame number: 00347553  Average reward: 2.5  Time taken: 29.5s\n",
      "Game number: 001800  Frame number: 00349854  Average reward: 3.3  Time taken: 25.9s\n",
      "Game number: 001810  Frame number: 00352220  Average reward: 3.0  Time taken: 31.7s\n",
      "Game number: 001820  Frame number: 00354345  Average reward: 2.2  Time taken: 27.3s\n",
      "Game number: 001830  Frame number: 00356640  Average reward: 2.7  Time taken: 33.7s\n",
      "Game number: 001840  Frame number: 00358997  Average reward: 2.9  Time taken: 34.6s\n",
      "Game number: 001850  Frame number: 00360955  Average reward: 2.1  Time taken: 25.5s\n",
      "Game number: 001860  Frame number: 00362839  Average reward: 1.8  Time taken: 24.9s\n",
      "Game number: 001870  Frame number: 00365418  Average reward: 3.2  Time taken: 39.8s\n",
      "Game number: 001880  Frame number: 00368001  Average reward: 3.7  Time taken: 52.1s\n",
      "Game number: 001890  Frame number: 00370107  Average reward: 2.4  Time taken: 35.2s\n",
      "Game number: 001900  Frame number: 00372165  Average reward: 2.2  Time taken: 27.3s\n",
      "Game number: 001910  Frame number: 00374456  Average reward: 2.9  Time taken: 28.4s\n",
      "Game number: 001920  Frame number: 00376923  Average reward: 3.4  Time taken: 41.2s\n",
      "Game number: 001930  Frame number: 00379470  Average reward: 3.5  Time taken: 36.0s\n",
      "Game number: 001940  Frame number: 00381992  Average reward: 3.4  Time taken: 29.7s\n",
      "Game number: 001950  Frame number: 00384465  Average reward: 3.7  Time taken: 42.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number: 001960  Frame number: 00386925  Average reward: 3.5  Time taken: 47.4s\n",
      "Game number: 001970  Frame number: 00389802  Average reward: 4.3  Time taken: 41.3s\n",
      "Game number: 001980  Frame number: 00392322  Average reward: 3.6  Time taken: 29.6s\n",
      "Game number: 001990  Frame number: 00395217  Average reward: 4.3  Time taken: 32.1s\n",
      "Game number: 002000  Frame number: 00397906  Average reward: 3.9  Time taken: 35.9s\n",
      "Evaluation score: 4.368421052631579\n",
      "Game number: 002010  Frame number: 00400693  Average reward: 3.9  Time taken: 43.8s\n",
      "Game number: 002020  Frame number: 00403377  Average reward: 3.8  Time taken: 43.1s\n",
      "Game number: 002030  Frame number: 00405964  Average reward: 3.8  Time taken: 39.7s\n",
      "Game number: 002040  Frame number: 00408940  Average reward: 4.5  Time taken: 61.0s\n",
      "Game number: 002050  Frame number: 00411808  Average reward: 4.3  Time taken: 64.7s\n",
      "Game number: 002060  Frame number: 00414087  Average reward: 2.8  Time taken: 40.8s\n",
      "Game number: 002070  Frame number: 00416602  Average reward: 3.3  Time taken: 39.0s\n",
      "Game number: 002080  Frame number: 00419380  Average reward: 4.0  Time taken: 70.2s\n",
      "Game number: 002090  Frame number: 00421813  Average reward: 4.0  Time taken: 33.6s\n",
      "Game number: 002100  Frame number: 00424426  Average reward: 4.1  Time taken: 34.5s\n",
      "Game number: 002110  Frame number: 00426758  Average reward: 3.1  Time taken: 31.6s\n",
      "Game number: 002120  Frame number: 00429259  Average reward: 3.6  Time taken: 46.7s\n",
      "Game number: 002130  Frame number: 00431919  Average reward: 3.8  Time taken: 46.3s\n",
      "Game number: 002140  Frame number: 00434413  Average reward: 3.3  Time taken: 57.0s\n",
      "Game number: 002150  Frame number: 00437292  Average reward: 4.1  Time taken: 30.2s\n",
      "Game number: 002160  Frame number: 00439595  Average reward: 2.8  Time taken: 28.9s\n",
      "Game number: 002170  Frame number: 00442178  Average reward: 3.5  Time taken: 32.0s\n",
      "Game number: 002180  Frame number: 00445001  Average reward: 4.2  Time taken: 25.0s\n",
      "Game number: 002190  Frame number: 00447867  Average reward: 5.3  Time taken: 42.6s\n",
      "Game number: 002200  Frame number: 00450578  Average reward: 4.2  Time taken: 47.4s\n",
      "Game number: 002210  Frame number: 00453592  Average reward: 4.7  Time taken: 36.6s\n",
      "Game number: 002220  Frame number: 00456231  Average reward: 3.9  Time taken: 53.2s\n",
      "Game number: 002230  Frame number: 00458942  Average reward: 4.2  Time taken: 34.2s\n",
      "Game number: 002240  Frame number: 00461525  Average reward: 3.5  Time taken: 40.9s\n",
      "Game number: 002250  Frame number: 00464625  Average reward: 5.1  Time taken: 34.7s\n",
      "Game number: 002260  Frame number: 00467195  Average reward: 3.7  Time taken: 35.7s\n",
      "Game number: 002270  Frame number: 00470166  Average reward: 4.5  Time taken: 49.0s\n",
      "Game number: 002280  Frame number: 00472837  Average reward: 3.8  Time taken: 38.5s\n",
      "Game number: 002290  Frame number: 00475718  Average reward: 4.7  Time taken: 30.9s\n",
      "Game number: 002300  Frame number: 00478711  Average reward: 4.8  Time taken: 43.3s\n",
      "Game number: 002310  Frame number: 00481804  Average reward: 4.9  Time taken: 40.3s\n",
      "Game number: 002320  Frame number: 00484197  Average reward: 3.1  Time taken: 28.1s\n",
      "Game number: 002330  Frame number: 00486977  Average reward: 4.5  Time taken: 35.7s\n",
      "Game number: 002340  Frame number: 00490065  Average reward: 4.7  Time taken: 34.8s\n",
      "Game number: 002350  Frame number: 00493843  Average reward: 6.4  Time taken: 49.9s\n",
      "Game number: 002360  Frame number: 00496846  Average reward: 5.2  Time taken: 42.6s\n",
      "Game number: 002370  Frame number: 00499599  Average reward: 4.1  Time taken: 31.6s\n",
      "Evaluation score: 4.857142857142857\n",
      "Game number: 002380  Frame number: 00502665  Average reward: 5.5  Time taken: 65.0s\n",
      "Game number: 002390  Frame number: 00505458  Average reward: 4.5  Time taken: 50.5s\n",
      "Game number: 002400  Frame number: 00508638  Average reward: 5.7  Time taken: 30.0s\n",
      "Game number: 002410  Frame number: 00511876  Average reward: 5.4  Time taken: 37.1s\n",
      "Game number: 002420  Frame number: 00514799  Average reward: 5.4  Time taken: 29.3s\n",
      "Game number: 002430  Frame number: 00517782  Average reward: 4.8  Time taken: 47.5s\n",
      "Game number: 002440  Frame number: 00520589  Average reward: 3.9  Time taken: 31.5s\n",
      "Game number: 002450  Frame number: 00523444  Average reward: 4.1  Time taken: 40.8s\n",
      "Game number: 002460  Frame number: 00526480  Average reward: 4.9  Time taken: 53.7s\n",
      "Game number: 002470  Frame number: 00529777  Average reward: 5.7  Time taken: 65.0s\n",
      "Game number: 002480  Frame number: 00532690  Average reward: 5.9  Time taken: 44.0s\n",
      "Game number: 002490  Frame number: 00535866  Average reward: 5.3  Time taken: 28.0s\n",
      "Game number: 002500  Frame number: 00539237  Average reward: 5.8  Time taken: 26.7s\n",
      "Game number: 002510  Frame number: 00542046  Average reward: 3.8  Time taken: 47.1s\n",
      "Game number: 002520  Frame number: 00544979  Average reward: 4.8  Time taken: 53.1s\n",
      "Game number: 002530  Frame number: 00548466  Average reward: 5.9  Time taken: 69.2s\n",
      "Game number: 002540  Frame number: 00551662  Average reward: 5.1  Time taken: 43.8s\n",
      "Game number: 002550  Frame number: 00555347  Average reward: 6.4  Time taken: 66.8s\n",
      "Game number: 002560  Frame number: 00558418  Average reward: 4.9  Time taken: 49.4s\n",
      "Game number: 002570  Frame number: 00561244  Average reward: 4.7  Time taken: 72.2s\n",
      "Game number: 002580  Frame number: 00564834  Average reward: 5.9  Time taken: 93.7s\n",
      "Game number: 002590  Frame number: 00568155  Average reward: 5.6  Time taken: 55.1s\n",
      "Game number: 002600  Frame number: 00571622  Average reward: 5.5  Time taken: 38.5s\n",
      "Game number: 002610  Frame number: 00574979  Average reward: 6.0  Time taken: 71.1s\n",
      "Game number: 002620  Frame number: 00578120  Average reward: 5.4  Time taken: 43.0s\n",
      "Game number: 002630  Frame number: 00581230  Average reward: 5.0  Time taken: 42.8s\n",
      "Game number: 002640  Frame number: 00584333  Average reward: 4.7  Time taken: 31.4s\n",
      "Game number: 002650  Frame number: 00587670  Average reward: 5.4  Time taken: 46.3s\n",
      "Game number: 002660  Frame number: 00591074  Average reward: 6.3  Time taken: 29.6s\n",
      "Game number: 002670  Frame number: 00594091  Average reward: 4.7  Time taken: 85.5s\n",
      "Game number: 002680  Frame number: 00597421  Average reward: 5.5  Time taken: 66.6s\n",
      "Game number: 002690  Frame number: 00600689  Average reward: 5.7  Time taken: 56.6s\n",
      "Evaluation score: 13.5\n",
      "Game number: 002700  Frame number: 00604644  Average reward: 8.0  Time taken: 112.7s\n",
      "Game number: 002710  Frame number: 00607762  Average reward: 5.9  Time taken: 82.0s\n",
      "Game number: 002720  Frame number: 00611514  Average reward: 7.2  Time taken: 59.5s\n",
      "Game number: 002730  Frame number: 00615110  Average reward: 6.4  Time taken: 79.6s\n",
      "Game number: 002740  Frame number: 00618945  Average reward: 7.0  Time taken: 98.8s\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "import time\n",
    "try:\n",
    "    with writer.as_default():\n",
    "        while frame_number < TOTAL_FRAMES:\n",
    "            # Training\n",
    "\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < FRAMES_BETWEEN_EVAL:\n",
    "                start_time = time.time()\n",
    "                game_wrapper.reset()\n",
    "                life_lost = True\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    # Get action\n",
    "                    action = agent.get_action(frame_number, game_wrapper.state)\n",
    "\n",
    "                    # Take step\n",
    "                    processed_frame, reward, terminal, life_lost = game_wrapper.step(action)\n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "\n",
    "                    # Add experience to replay memory\n",
    "                    agent.add_experience(action=action,\n",
    "                                         frame=processed_frame[:, :, 0],\n",
    "                                         reward=reward, clip_reward=CLIP_REWARD,\n",
    "                                         terminal=life_lost)\n",
    "\n",
    "                    # Update agent\n",
    "                    if frame_number % UPDATE_FREQ == 0 and agent.replay_buffer.count > MIN_REPLAY_BUFFER_SIZE:\n",
    "                        loss, _ = agent.learn(BATCH_SIZE, gamma=DISCOUNT_FACTOR, frame_number=frame_number, priority_scale=PRIORITY_SCALE)\n",
    "                        loss_list.append(loss)\n",
    "\n",
    "                    # Update target network\n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > MIN_REPLAY_BUFFER_SIZE:\n",
    "                        agent.update_target_network()\n",
    "\n",
    "                    # Break the loop when the game is over\n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "\n",
    "                # Output the progress every 10 games\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # Write to TensorBoard\n",
    "                    if WRITE_TENSORBOARD:\n",
    "                        tf.summary.scalar('Reward', np.mean(rewards[-10:]), frame_number)\n",
    "                        tf.summary.scalar('Loss', np.mean(loss_list[-100:]), frame_number)\n",
    "                        writer.flush()\n",
    "\n",
    "                    print(f'Game number: {str(len(rewards)).zfill(6)}  Frame number: {str(frame_number).zfill(8)}  Average reward: {np.mean(rewards[-10:]):0.1f}  Time taken: {(time.time() - start_time):.1f}s')\n",
    "\n",
    "            # Evaluation every `FRAMES_BETWEEN_EVAL` frames\n",
    "            terminal = True\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "\n",
    "            for _ in range(EVAL_LENGTH):\n",
    "                if terminal:\n",
    "                    game_wrapper.reset(evaluation=True)\n",
    "                    life_lost = True\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "\n",
    "                # Breakout requires a \"fire\" action (action #1) to start the\n",
    "                # game each time a life is lost.\n",
    "                # Otherwise, the agent would sit around doing nothing.\n",
    "                action = 1 if life_lost else agent.get_action(frame_number, game_wrapper.state, evaluation=True)\n",
    "\n",
    "                # Step action\n",
    "                _, reward, terminal, life_lost = game_wrapper.step(action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "\n",
    "                # On game-over\n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "\n",
    "            if len(eval_rewards) > 0:\n",
    "                final_score = np.mean(eval_rewards)\n",
    "            else:\n",
    "                # In case the game is longer than the number of frames allowed\n",
    "                final_score = episode_reward_sum\n",
    "            # Print score and write to tensorboard\n",
    "            print('Evaluation score:', final_score)\n",
    "            if WRITE_TENSORBOARD:\n",
    "                tf.summary.scalar('Evaluation score', final_score, frame_number)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model\n",
    "            if len(rewards) > 300 and SAVE_PATH is not None:\n",
    "                agent.save(f'{SAVE_PATH}/save-{str(frame_number).zfill(8)}', frame_number=frame_number, rewards=rewards, loss_list=loss_list)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\nTraining exited early.')\n",
    "    writer.close()\n",
    "\n",
    "    if SAVE_PATH is None:\n",
    "        try:\n",
    "            SAVE_PATH = input('Would you like to save the trained model? If so, type in a save path, otherwise, interrupt with ctrl+c. ')\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nExiting...')\n",
    "\n",
    "    if SAVE_PATH is not None:\n",
    "        print('Saving...')\n",
    "        agent.save(f'{SAVE_PATH}/save-{str(frame_number).zfill(8)}', frame_number=frame_number, rewards=rewards, loss_list=loss_list)\n",
    "        print('Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "\n",
    "# Create environment\n",
    "game_wrapper = GameWrapper(ENV_NAME, MAX_NOOP_STEPS)\n",
    "print(\"The environment has the following {} actions: {}\".format(game_wrapper.env.action_space.n, game_wrapper.env.unwrapped.get_action_meanings()))\n",
    "\n",
    "# Create agent\n",
    "MAIN_DQN = build_q_network(game_wrapper.env.action_space.n, LEARNING_RATE, input_shape=INPUT_SHAPE)\n",
    "TARGET_DQN = build_q_network(game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE)\n",
    "\n",
    "replay_buffer = ReplayBuffer(size=MEM_SIZE, batch_size=BATCH_SIZE, input_shape=INPUT_SHAPE)\n",
    "agent = Agent(MAIN_DQN, TARGET_DQN, replay_buffer, game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE)\n",
    "\n",
    "print('Loading model...')\n",
    "agent.load('/content/breakout-saves/save-00250392')\n",
    "print('Loaded')\n",
    "\n",
    "terminal = True\n",
    "eval_rewards = []\n",
    "evaluate_frame_number = 0\n",
    "\n",
    "for frame in range(EVAL_LENGTH):\n",
    "    if terminal:\n",
    "        game_wrapper.reset(evaluation=True)\n",
    "        life_lost = True\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "\n",
    "    # Breakout require a \"fire\" action (action #1) to start the\n",
    "    # game each time a life is lost.\n",
    "    # Otherwise, the agent would sit around doing nothing.\n",
    "    action = 1 if life_lost else agent.get_action(0, game_wrapper.state, evaluation=True)\n",
    "\n",
    "    # Step action\n",
    "    _, reward, terminal, life_lost = game_wrapper.step(action, render_mode='human')\n",
    "    evaluate_frame_number += 1\n",
    "    episode_reward_sum += reward\n",
    "\n",
    "    # On game-over\n",
    "    if terminal:\n",
    "        print(f'Game over, reward: {episode_reward_sum}, frame: {frame}/{EVAL_LENGTH}')\n",
    "        eval_rewards.append(episode_reward_sum)\n",
    "\n",
    "print('Average reward:', np.mean(eval_rewards) if len(eval_rewards) > 0 else episode_reward_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
